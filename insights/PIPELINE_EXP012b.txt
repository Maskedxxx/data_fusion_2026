╔══════════════════════════════════════════════════════════════════════════════════╗
║           EXP-012b: ВЫИГРЫШНЫЙ ПАЙПЛАЙН — PUBLIC LB 0.8510                     ║
║                                                                                ║
║  Трёхуровневая архитектура:                                                    ║
║  L0.5 (Optuna) → L1 (41 XGBoost) → L2a (XGBoost) + L2b (PyTorch NN) → Бленд  ║
║  Метрика: macro ROC-AUC по 41 таргету                                          ║
╚══════════════════════════════════════════════════════════════════════════════════╝


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 0: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
═══════════════════════════════════════════════════════════════════════════════════

  train_main.parquet          train_extra.parquet         train_target.parquet
  ┌──────────────────┐        ┌──────────────────┐        ┌──────────────────┐
  │ 750k строк       │        │ 750k строк       │        │ 750k строк       │
  │ customer_id      │        │ customer_id      │        │ customer_id      │
  │ 67 cat_feature_* │        │ 2241 числовых    │        │ 41 target_*_*    │
  │ 132 num_feature_*│        │ (обфусцировано)  │        │ (0 или 1)        │
  │ = 199 признаков  │        │                  │        │                  │
  └────────┬─────────┘        └────────┬─────────┘        └────────┬─────────┘
           │                           │                           │
           └─────────┬─────────────────┘                           │
                     ▼                                             ▼
              LEFT JOIN по customer_id                     drop customer_id
                     │                                             │
                     ▼                                             ▼
           ┌──────────────────┐                          ┌──────────────────┐
           │ X_train_np       │                          │ y_train_np       │
           │ (750000, 2440)   │                          │ (750000, 41)     │
           │ float32          │                          │ int (0/1)        │
           └──────────────────┘                          └──────────────────┘

  test_main.parquet           test_extra.parquet
  ┌──────────────────┐        ┌──────────────────┐
  │ 250k строк       │        │ 250k строк       │
  │ 199 признаков    │        │ 2241 числовых    │
  └────────┬─────────┘        └────────┬─────────┘
           └─────────┬─────────────────┘
                     ▼
           ┌──────────────────┐        ┌──────────────────┐
           │ X_test_np        │        │ test_customer_ids │
           │ (250000, 2440)   │        │ (250000,)         │
           │ float32          │        │ для сабмита       │
           └──────────────────┘        └──────────────────┘

  Нюансы:
  • astype(np.float32) — категориальные тоже переводятся во float для XGBoost GPU
  • Все пропуски (NaN) сохраняются — XGBoost обрабатывает их нативно (learn_missing)
  • gc.collect() — освобождаем pandas DataFrames (~3 GB)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 0.5: OPTUNA — PER-TARGET ГИПЕРПАРАМЕТРЫ + FEATURE SELECTION
═══════════════════════════════════════════════════════════════════════════════════

  Выполняется ОДИН РАЗ офлайн. Результат переиспользуется в шагах 1-3.

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                                                                           │
  │  Подвыборка: 100k из 750k (random_state=42)                              │
  │  Для каждого таргета t (0..40):                                           │
  │                                                                           │
  │  ┌─────────────────────────────────────────────────────────────────────┐   │
  │  │  ЭТАП A: FEATURE SELECTION (черновик)                               │   │
  │  │                                                                    │   │
  │  │  XGBoost(100k, 2440 фичей, 100 iter, depth=6) → importance(gain)  │   │
  │  │  Сортировка по gain ↓ → Cumulative Gain 95%                       │   │
  │  │                                                                    │   │
  │  │  Пример:                                                           │   │
  │  │  ┌────────────────┬────────┬──────────────┐                       │   │
  │  │  │ Фича           │ Gain   │ Cumul. %     │                       │   │
  │  │  ├────────────────┼────────┼──────────────┤                       │   │
  │  │  │ num_feature_22 │ 1177   │   12%  ✓     │                       │   │
  │  │  │ extra_feat_156 │  243   │   14%  ✓     │                       │   │
  │  │  │ ...            │ ...    │   ...  ✓     │                       │   │
  │  │  │ extra_feat_834 │    2   │   95%  ✓ ← СТОП                     │   │
  │  │  │ extra_feat_901 │    1   │  95.1% ✗ отсечено                   │   │
  │  │  └────────────────┴────────┴──────────────┘                       │   │
  │  │                                                                    │   │
  │  │  Результат: список имён фичей для таргета t                       │   │
  │  │  Разные таргеты → разное кол-во (176..1243 фичей)                 │   │
  │  └─────────────────────────────────────────────────────────────────────┘   │
  │                                                                           │
  │  ┌─────────────────────────────────────────────────────────────────────┐   │
  │  │  ЭТАП B: OPTUNA (подбор гиперпараметров)                           │   │
  │  │                                                                    │   │
  │  │  20 trials × 3-fold StratifiedKFold на 100k С ОТОБРАННЫМИ фичами  │   │
  │  │                                                                    │   │
  │  │  Пространство поиска:                                              │   │
  │  │  ┌────────────────────┬─────────────┬──────────────────────────┐   │   │
  │  │  │ Параметр           │ Диапазон    │ Смысл                    │   │   │
  │  │  ├────────────────────┼─────────────┼──────────────────────────┤   │   │
  │  │  │ max_depth          │ [3, 8]      │ Глубина дерева           │   │   │
  │  │  │ learning_rate      │ [0.01, 0.1] │ Шаг обучения             │   │   │
  │  │  │ colsample_bytree   │ [0.3, 0.9]  │ Доля фичей на дерево     │   │   │
  │  │  │ min_child_weight   │ [1, 20]     │ Мин. вес в листе         │   │   │
  │  │  │ reg_alpha          │ [1e-8, 10]  │ L1 регуляризация         │   │   │
  │  │  │ reg_lambda         │ [1e-8, 10]  │ L2 регуляризация         │   │   │
  │  │  │ n_rounds           │ [200, 2000] │ Кол-во деревьев          │   │   │
  │  │  └────────────────────┴─────────────┴──────────────────────────┘   │   │
  │  │                                                                    │   │
  │  │  Optuna максимизирует: mean(AUC по 3 фолдам)                      │   │
  │  │                                                                    │   │
  │  │  Паттерны найденных параметров:                                    │   │
  │  │  • lr = 0.01-0.02 (vs default 0.05) — медленнее, но точнее        │   │
  │  │  • colsample = 0.30-0.89 — варьируется по таргетам                │   │
  │  │  • mcw = 1-20 — слабые таргеты хотят 10-20 (регуляризация)        │   │
  │  │  • n_rounds = 500-1500                                             │   │
  │  │                                                                    │   │
  │  │  КЛЮЧЕВОЙ ВЫВОД: слабые таргеты страдают от ПЕРЕОБУЧЕНИЯ,         │   │
  │  │  не недообучения. Им нужна больше регуляризации (ниже lr,          │   │
  │  │  агрессивнее colsample, выше mcw), а не больше итераций.           │   │
  │  └─────────────────────────────────────────────────────────────────────┘   │
  │                                                                           │
  └───────────────────────────────────────────────────────────────────────────┘

  РЕЗУЛЬТАТ ШАГА 0.5:

  optuna_best_params.json — 41 набор гиперпараметров
  ┌──────────────────────────────────────────────────────────────────┐
  │ {                                                                │
  │   "target_1_1": {                                                │
  │     "max_depth": 5, "learning_rate": 0.015,                      │
  │     "colsample_bytree": 0.62, "min_child_weight": 8,             │
  │     "reg_alpha": 0.034, "reg_lambda": 2.15, "n_rounds": 890,     │
  │     "best_auc": 0.8231   ← метаданные, удаляются перед обучением │
  │   },                                                             │
  │   "target_2_8": {                                                │
  │     "max_depth": 3, "learning_rate": 0.012,                      │
  │     "colsample_bytree": 0.34, "min_child_weight": 18, ...        │
  │   },                                                             │
  │   ...  (41 запись)                                                │
  │ }                                                                │
  └──────────────────────────────────────────────────────────────────┘

  xgb_best_features_optuna.json — 41 список отобранных фичей
  ┌──────────────────────────────────────────────────────────────────┐
  │ {                                                                │
  │   "target_1_1": ["num_feature_22", "extra_feature_156", ...],    │ 898 фичей
  │   "target_2_8": ["num_feature_22", "extra_feature_89", ...],     │ 176 фичей
  │   ...  (41 запись)                                                │
  │ }                                                                │
  └──────────────────────────────────────────────────────────────────┘

  Время: ~4-6 часов (Colab T4, 41 таргет × 20 trials × 3-fold)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 1: УРОВЕНЬ L1 — OOF ПРЕДСКАЗАНИЯ (750k × 41)
═══════════════════════════════════════════════════════════════════════════════════

  Цикл повторяется 41 раз. Каждый таргет обучается НЕЗАВИСИМО,
  со СВОИМИ параметрами (из Optuna) и СВОИМИ фичами (из FS).

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                     ДЛЯ КАЖДОГО ТАРГЕТА t (1..41):                        │
  │                                                                           │
  │  params  ← optuna_best_params.json[t]   (индивидуальные!)                 │
  │  feats   ← xgb_best_features_optuna.json[t]  (176-1243 фичей)            │
  │  X_sel   ← X_train_np[:, feat_indices]  (750k × feats[t])                │
  │                                                                           │
  │  5-Fold StratifiedKFold (shuffle=True, random_state=42)                   │
  │  Stratified = сохраняет пропорцию 0/1 в каждом фолде                     │
  │  Критично для редких таргетов (target_2_8: 83 позитива = ~17/фолд)       │
  │                                                                           │
  │  ┌─────────────────────────────────────────────────────────────────────┐   │
  │  │                                                                     │   │
  │  │  ФОЛД 1:   [████████████████████████████████████████]  750k        │   │
  │  │             ├─TRAIN (600k)───────────────┤├─VAL (150k)┤            │   │
  │  │                                                                     │   │
  │  │  ФОЛД 2:   [████████████████████████████████████████]  750k        │   │
  │  │             ├─VAL─┤├─TRAIN (600k)────────────────────┤             │   │
  │  │                                                                     │   │
  │  │  ФОЛД 3:   [████████████████████████████████████████]  750k        │   │
  │  │             ├─TRAIN─┤├─VAL─┤├─TRAIN──────────────────┤             │   │
  │  │                                                                     │   │
  │  │  ФОЛД 4:   [████████████████████████████████████████]  750k        │   │
  │  │             ├─TRAIN──────────────┤├─VAL─┤├─TRAIN─────┤             │   │
  │  │                                                                     │   │
  │  │  ФОЛД 5:   [████████████████████████████████████████]  750k        │   │
  │  │             ├─TRAIN (600k)───────────────────┤├─VAL──┤             │   │
  │  │                                                                     │   │
  │  │  Для каждого фолда:                                                 │   │
  │  │  1. dtrain = DMatrix(X_sel[trn_idx])  ← 600k × feats[t]            │   │
  │  │  2. dval   = DMatrix(X_sel[val_idx])                                │   │
  │  │  3. xgb.train(params[t], dtrain, n_rounds[t])                      │   │
  │  │     • Параметры ИНДИВИДУАЛЬНЫЕ для этого таргета                    │   │
  │  │     • Без early stopping — фиксированное кол-во итераций            │   │
  │  │  4. model.predict(dval) → OOF[val_idx, t]                          │   │
  │  │                                                                     │   │
  │  │  КЛЮЧЕВОЙ МОМЕНТ: каждый клиент попадает в VAL ровно 1 раз         │   │
  │  │  → его предсказание "честное" (модель его НЕ ВИДЕЛА)                │   │
  │  └─────────────────────────────────────────────────────────────────────┘   │
  │                                                                           │
  │  oof_predictions[val_idx, t] = model.predict(dval)                        │
  │  Чекпоинт: np.save() каждые 5 таргетов (защита от Colab-отключений)      │
  └───────────────────────────────────────────────────────────────────────────┘

  РЕЗУЛЬТАТ ШАГА 1:

  oof_predictions (OOF-матрица)
  ┌──────────────────────────────────────────────────────────────┐
  │         target_1_1  target_1_2  ...  target_9_6  target_10_1│
  │ клиент 1   0.023       0.156   ...    0.341       0.089     │
  │ клиент 2   0.871       0.034   ...    0.112       0.456     │
  │ ...        ...         ...     ...    ...         ...       │
  │ (750000, 41) float32                                        │
  │ Каждое значение — "честная" вероятность                      │
  └──────────────────────────────────────────────────────────────┘
  Сохраняется: xgb_oof_optuna.npy

  OOF Macro AUC = 0.8407
  Время: ~135 мин (Colab T4)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 2: FULL TRAIN + TEST INFERENCE (L1 предсказания на test)
═══════════════════════════════════════════════════════════════════════════════════

  Для каждого из 41 таргетов:

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  params    ← optuna_best_params.json[t]                                │
  │  feats     ← xgb_best_features_optuna.json[t]                         │
  │  n_rounds  ← params[t].n_rounds × 1.2   ← больше итераций на полных  │
  │                                            данных (vs 4/5 в OOF)      │
  │                                                                        │
  │  X_train_sel = X_train_np[:, feat_indices]    (750k × feats[t])       │
  │  X_test_sel  = X_test_np[:, feat_indices]     (250k × feats[t])       │
  │                                                                        │
  │  XGBoost train: ВСЕ 750k, n_rounds × 1.2, без early stopping         │
  │         │                                                              │
  │         ▼                                                              │
  │      model ──── predict(X_test_sel) ──→ test_preds[:, t]              │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  Зачем отдельный шаг (а не модели из шага 1)?
  • В шаге 1 модели на 600k (4/5) — для OOF
  • Здесь на ВСЕ 750k — максимум данных для теста
  • Множитель ×1.2: компенсация (+20% данных → +20% итераций)

  РЕЗУЛЬТАТ ШАГА 2:

  test_predictions (L1 test)
  ┌──────────────────────────────────────────────────────────────┐
  │ (250000, 41) float32                                        │
  └──────────────────────────────────────────────────────────────┘
  Сохраняется: xgb_test_optuna.npy

  Время: ~22 мин (Colab T4)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 3: УРОВЕНЬ L2a — XGBoost МЕТА-МОДЕЛЬ (СТЕКИНГ)
═══════════════════════════════════════════════════════════════════════════════════

  Входные данные L2 — это ВЫХОДЫ L1:

  X_meta_train = xgb_oof_optuna.npy    (750000, 41)  ← OOF предсказания
  X_meta_test  = xgb_test_optuna.npy   (250000, 41)  ← L1 test предсказания
  y_train      = train_target          (750000, 41)  ← настоящие метки

  Для каждого из 41 таргетов:

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  X_meta_train (750k, 41)                                               │
  │  ┌────────────────────────────────────────────────────┐                 │
  │  │ Признаки мета-модели — OOF-вероятности:            │                 │
  │  │                                                    │                 │
  │  │ OOF_1_1  OOF_1_2  ...  OOF_9_6  OOF_10_1         │                 │
  │  │  0.023    0.156   ...   0.341    0.089              │                 │
  │  │  0.871    0.034   ...   0.112    0.456              │                 │
  │  │                                                    │                 │
  │  │ Каждая строка — "портрет" клиента глазами L1:      │                 │
  │  │ вероятности покупки КАЖДОГО из 41 продуктов        │                 │
  │  └────────────────────────────────────────────────────┘                 │
  │         │                                                              │
  │         ▼                                                              │
  │  XGBClassifier — мета-модель                                           │
  │  ┌─────────────────────────────────────────┐                           │
  │  │ max_depth: 2         ← МЕЛКИЕ деревья!  │                           │
  │  │ n_estimators: 100    ← мало деревьев    │                           │
  │  │ lr: 0.05                                │                           │
  │  │ subsample: 0.8                          │                           │
  │  │ colsample_bytree: 0.8                   │                           │
  │  │ min_child_weight: 10                    │                           │
  │  │                                         │                           │
  │  │ ПОЧЕМУ depth=2?                         │                           │
  │  │ • 41 фича — мало для глубоких деревьев  │                           │
  │  │ • Нужны простые правила вида:            │                           │
  │  │   "если OOF_3_1 > 0.5 И OOF_3_2 > 0.3  │                           │
  │  │    → target_3_3 вероятен"                │                           │
  │  │ • 100 деревьев × depth 2 = 100 правил   │                           │
  │  └─────────────────────────────────────────┘                           │
  │                                                                        │
  │  5-Fold KFold OOF → L2_OOF (750k, 41)  ← для NN и валидации          │
  │  5-Fold avg predict → L2_Test (250k, 41)                               │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  ЧТО ВИДИТ МЕТА-МОДЕЛЬ (пример для target_3_3):

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  Клиент А:  OOF_3_1=0.82  OOF_3_2=0.91  OOF_3_5=0.78  OOF_10_1=0.12 │
  │             ↑ группа 3 — все высокие       ↑ антагонист — низкий        │
  │             → мета-модель: target_3_3 ≈ 0.85 (пакетная покупка!)        │
  │                                                                        │
  │  Клиент Б:  OOF_3_1=0.05  OOF_3_2=0.03  OOF_3_5=0.02  OOF_10_1=0.89 │
  │             ↑ группа 3 — все низкие        ↑ антагонист — высокий       │
  │             → мета-модель: target_3_3 ≈ 0.01                            │
  │                                                                        │
  │  КОРРЕЛЯЦИИ, КОТОРЫЕ УЧИТ МЕТА-МОДЕЛЬ:                                 │
  │  • Группы 3-7: внутригрупповая корреляция (пакетные покупки)            │
  │  • target_10_1: антагонист (100% исключение с остальными 40)            │
  │  • Бандлы: 6_4→6_5 (lift 127x), 5_1→5_2 (lift 106x)                   │
  │  • Редкие таргеты получают сигнал от частых связанных                   │
  │                                                                        │
  │  ДО стекинга:  target_5_2 AUC = 0.71                                   │
  │  ПОСЛЕ:        target_5_2 AUC = 0.79 (+0.08!)                          │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  РЕЗУЛЬТАТ ШАГА 3:

  L2_OOF  (750000, 41) — для обучения NN и валидации
  L2_Test (250000, 41) — предсказания XGB L2

  L2 OOF Macro AUC = 0.8423
  Время: ~1 мин


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 4: УРОВЕНЬ L2b — PyTorch NN МЕТА-МОДЕЛЬ
═══════════════════════════════════════════════════════════════════════════════════

  Вторая L2 модель — нейросеть. Даёт ДРУГИЕ ошибки чем XGB L2.

  ┌──────────────────────────────────────────────────────────────────────────┐
  │  ПРЕДОБРАБОТКА: Logit-трансформация                                    │
  │                                                                        │
  │  OOF_raw (0.023)  →  logit = log(0.023 / 0.977) = -3.75               │
  │  OOF_raw (0.871)  →  logit = log(0.871 / 0.129) = +1.91               │
  │                                                                        │
  │  Зачем: вероятности [0,1] сжаты у краёв. Logit растягивает →           │
  │  NN видит линейные различия. Без logit NN плохо различает              │
  │  0.01 и 0.001 (оба ≈ 0), а в logit это -4.6 и -6.9 (разница 2.3!)    │
  │                                                                        │
  │  X_train_logits = log(clip(OOF) / (1 - clip(OOF)))    (750k, 41)      │
  │  X_test_logits  = log(clip(Test) / (1 - clip(Test)))   (250k, 41)      │
  └──────────────────────────────────────────────────────────────────────────┘

  ┌──────────────────────────────────────────────────────────────────────────┐
  │  АРХИТЕКТУРА: MultiLabelMetaNN_Res (skip connection)                   │
  │                                                                        │
  │  Optuna нашла: hidden=512, drop1=0.1907, drop2=0.2226,                │
  │                lr=0.02558, wd=1.157e-5                                  │
  │                                                                        │
  │  ┌──────────────────────────────────────────────────────────┐          │
  │  │                                                          │          │
  │  │  Input: logit(OOF)  (batch, 41)                         │          │
  │  │    │                                                     │          │
  │  │    ├────────────────────────────────┐                    │          │
  │  │    │                                │ skip connection    │          │
  │  │    ▼                                │ (вход напрямую     │          │
  │  │  Linear(41 → 512)                   │  в классификатор)  │          │
  │  │    │                                │                    │          │
  │  │  BatchNorm1d(512)                   │                    │          │
  │  │    │                                │                    │          │
  │  │  SiLU (activation)                  │                    │          │
  │  │    │                                │                    │          │
  │  │  Dropout(0.1907)                    │                    │          │
  │  │    │                                │                    │          │
  │  │    ▼                                │                    │          │
  │  │  Linear(512 → 256)                  │                    │          │
  │  │    │                                │                    │          │
  │  │  BatchNorm1d(256)                   │                    │          │
  │  │    │                                │                    │          │
  │  │  SiLU (activation)                  │                    │          │
  │  │    │                                │                    │          │
  │  │  Dropout(0.2226)                    │                    │          │
  │  │    │                                │                    │          │
  │  │    ▼                                │                    │          │
  │  │  concat(256, 41) = 297  ◄───────────┘                    │          │
  │  │    │                                                     │          │
  │  │    ▼                                                     │          │
  │  │  Linear(297 → 41)                                        │          │
  │  │    │                                                     │          │
  │  │    ▼                                                     │          │
  │  │  Sigmoid → вероятности (batch, 41)                       │          │
  │  │                                                          │          │
  │  └──────────────────────────────────────────────────────────┘          │
  │                                                                        │
  │  ЗАЧЕМ SKIP CONNECTION?                                                │
  │  • Без skip: 41→512→256→41. NN может "забыть" исходный сигнал         │
  │  • С skip: классификатор видит И обработанные фичи (256), И сырые (41)│
  │  • Результат: +0.0005 LB (0.8505 → 0.8510)                            │
  │                                                                        │
  │  ЗАЧЕМ NN ВООБЩЕ, ЕСЛИ ЕСТЬ XGB L2?                                    │
  │  • XGB depth=2 = ступенчатые правила (if A>x then...)                  │
  │  • NN = гладкие комбинации (0.3*A + 0.7*B - 0.1*C)                    │
  │  • NN лучше видит: линейные тренды между таргетами,                    │
  │    плавные переходы, комбинации из 3+ таргетов одновременно             │
  │  • XGB лучше видит: точные пороги, взаимодействия 2 таргетов            │
  │  • Бленд = лучшее из обоих миров                                       │
  │                                                                        │
  │  ОБУЧЕНИЕ:                                                             │
  │  • AdamW: lr=0.002558 (lr/10), weight_decay=1.157e-5                   │
  │  • OneCycleLR: max_lr=0.02558, cosine annealing                        │
  │  • BCE with Logits Loss (multi-label)                                  │
  │  • Batch size: 4096                                                    │
  │  • 100 эпох, full train на 750k (без fold CV)                          │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  РЕЗУЛЬТАТ ШАГА 4:

  NN_Test (250000, 41) — предсказания NN L2
  Время: ~40 сек (A100) / ~2 мин (T4)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 5: БЛЕНД + ФОРМИРОВАНИЕ САБМИТА
═══════════════════════════════════════════════════════════════════════════════════

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  L2_Test (250k, 41)              NN_Test (250k, 41)                    │
  │  XGBoost depth=2                  PyTorch skip conn                    │
  │  ступенчатые правила              гладкие комбинации                   │
  │       │                                │                               │
  │       │        ┌──────────────┐        │                               │
  │       └───────→│  60% + 40%   │◄───────┘                               │
  │                └──────┬───────┘                                        │
  │                       │                                                │
  │                       ▼                                                │
  │              Final (250k, 41)                                          │
  │              = 0.6 × L2_Test + 0.4 × NN_Test                          │
  │                                                                        │
  │  Почему 60/40?                                                         │
  │  • XGB L2 OOF = 0.8423 (сильнее), NN L2 OOF = 0.8382 (слабее)        │
  │  • Но бленд > каждого по отдельности (разные ошибки)                   │
  │  • 60/40 найден эмпирически на OOF                                    │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  submission = DataFrame:
  ┌─────────────┬──────────────┬──────────────┬───┬───────────────┐
  │ customer_id  │ predict_1_1  │ predict_1_2  │...│ predict_10_1  │
  ├─────────────┼──────────────┼──────────────┼───┼───────────────┤
  │ 12345 (int32)│ 0.0341 (f64) │ 0.1782 (f64) │   │ 0.1234 (f64)  │
  └─────────────┴──────────────┴──────────────┴───┴───────────────┘

  ВАЖНО:
  • dtype: float64 (платформа отклоняет float32!)
  • customer_id: int32 (НЕ int64)
  • Колонки: predict_X_Y (НЕ predict_target_X_Y!)
  • Формат: parquet


═══════════════════════════════════════════════════════════════════════════════════
 ОБЩАЯ СХЕМА ПОТОКА ДАННЫХ
═══════════════════════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                     УРОВЕНЬ 0.5: OPTUNA (офлайн, один раз)               │
  │                                                                           │
  │  100k подвыборка → 41 × (FS + 20 trials × 3-fold)                       │
  │  → optuna_best_params.json + xgb_best_features_optuna.json               │
  └────────────────────────────────┬──────────────────────────────────────────┘
                                   │ params + features
                                   ▼
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                     УРОВЕНЬ L1: 41 XGBoost (per-target)                  │
  │                                                                           │
  │ TRAIN (750k, 2440)                          TEST (250k, 2440)             │
  │       │                                           │                       │
  │       │   ┌──────────────────────────────┐        │                       │
  │       │   │  Per-target Optuna params    │        │                       │
  │       │   │  Per-target Feature Select.  │        │                       │
  │       │   │  176-1243 фичей на таргет    │        │                       │
  │       │   └──────────────┬───────────────┘        │                       │
  │       ▼                  ▼                        ▼                       │
  │  ┌─────────────┐  ┌──────────────┐          ┌─────────────┐              │
  │  │ 5-Fold OOF  │  │  Full Train  │          │             │              │
  │  │ per-target  │  │  750k × 1.2  │──predict─→│ L1 test     │              │
  │  │ Optuna iter │  │  Optuna iter │          │ (250k, 41)  │              │
  │  └──────┬──────┘  └──────────────┘          └──────┬──────┘              │
  │         │ OOF (750k, 41)                           │                      │
  │         │ AUC = 0.8407                             │                      │
  └────────┬───────────────────────────────────────────┬──────────────────────┘
           │                                           │
           ▼                                           ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                     УРОВЕНЬ L2a: XGBoost стекинг                        │
  │                                                                          │
  │  X = 41 OOF фичи → XGBClassifier(depth=2, 100 iter) × 41 таргет        │
  │  5-Fold OOF → L2_OOF (750k, 41)    predict → L2_Test (250k, 41)        │
  │  AUC = 0.8423                                                            │
  └────────┬──────────────────────────────────────────┬──────────────────────┘
           │ L2_OOF                                   │ L2_Test
           ▼                                          │
  ┌────────────────────────────────────────────────────┤
  │               УРОВЕНЬ L2b: PyTorch NN             │
  │                                                    │
  │  logit(OOF) → NN (41→512→256+41→41, skip conn)   │
  │  Full train 100 эпох → predict → NN_Test          │
  └────────────────────────────────────┬───────────────┘
                                       │ NN_Test
                                       ▼
  ┌──────────────────────────────────────────────────────────────────────────┐
  │                          БЛЕНД                                          │
  │                                                                        │
  │  Final = 0.6 × L2_Test + 0.4 × NN_Test                                │
  │                                                                        │
  │  ═══════════════════════════════════                                    │
  │  ║  PUBLIC LB: 0.8510            ║                                     │
  │  ═══════════════════════════════════                                    │
  └──────────────────────────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════════
 ПОЧЕМУ OOF, А НЕ ОБЫЧНЫЕ ПРЕДСКАЗАНИЯ?
═══════════════════════════════════════════════════════════════════════════════════

  ПРОБЛЕМА: если обучить L1 на train и предсказать train для мета-фичей

  ┌────────────────────────────────────────────────────────────────────────┐
  │  L1 обучается на train → предсказывает train → AUC ~0.99 (переобучен)│
  │  L2 обучается на этих "идеальных" предсказаниях                      │
  │  L2 думает: "если L1 говорит >0.9 — это точно позитив"               │
  │  НО на тесте L1 предсказывает хуже → L2 ломается                    │
  │                                                                      │
  │  Результат: УТЕЧКА (data leakage), мета-модель переобучена           │
  └────────────────────────────────────────────────────────────────────────┘

  РЕШЕНИЕ: Out-of-Fold (OOF)

  ┌────────────────────────────────────────────────────────────────────────┐
  │  L1 обучается на 4/5 train → предсказывает 1/5 (которые НЕ видел)   │
  │  OOF предсказания имеют такой же уровень ошибки, как на тесте        │
  │  L2 обучается на "реалистичных" предсказаниях                        │
  │  L2 корректно учит: "L1 ошибается на ±X, учитываю это"              │
  │                                                                      │
  │  Результат: нет утечки, мета-модель генерализуется                   │
  └────────────────────────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════════
 КЛЮЧЕВЫЕ НЮАНСЫ И ТРЮКИ
═══════════════════════════════════════════════════════════════════════════════════

  1. PER-TARGET OPTUNA PARAMS
     • Каждый из 41 таргетов имеет СВОИ гиперпараметры
     • Редкие (target_2_8): depth=3, lr=0.012, mcw=18 — агрессивная регуляризация
     • Частые (target_8_1): depth=7, lr=0.02, mcw=3 — больше ёмкости
     • Прирост: OOF +0.0055, LB +0.0027

  2. PER-TARGET FEATURE SELECTION
     • Cumulative Gain 95%: оставляем фичи покрывающие 95% суммарного gain
     • Разные таргеты → разное кол-во (176..1243 фичей)
     • target_2_8 (83 позитива): 176 фичей — только самые сильные
     • target_8_1 (76k позитивов): 1243 фичи — может извлечь больше
     • Без FS: OOF 0.773 vs 0.835. Feature selection КРИТИЧНА!

  3. SKIP CONNECTION В NN
     • Вход (41 logit) подаётся напрямую в классификатор вместе с hidden (256)
     • NN не может "забыть" исходный сигнал
     • +0.0005 LB

  4. LOGIT-ТРАНСФОРМАЦИЯ
     • NN получает logit(p) вместо p
     • Растягивает экстремальные значения: 0.01 → -4.6, 0.001 → -6.9
     • NN лучше различает нюансы в хвостах распределения

  5. MULTI-TASK NN
     • Одна сеть предсказывает все 41 таргет одновременно
     • Учит скрытые связи: пакеты (группы 3-7), антагонист (10_1)
     • vs 41 отдельная NN: shared representation мощнее

  6. БЛЕНД 60/40
     • XGB L2 (0.8423) + NN L2 (0.8382) → бленд > каждого
     • Разные ошибки: XGB ступенчатые, NN гладкие
     • +0.0033 LB (0.8472 → 0.8505)

  7. ЧЕКПОИНТЫ + ВОССТАНОВЛЕНИЕ
     • np.save() каждые 5 таргетов (защита от Colab-отключений)
     • Можно перезапустить и продолжить с последнего чекпоинта

  8. ТИПЫ ДАННЫХ САБМИТА
     • float64 — платформа отклоняет float32!
     • customer_id: int32 (НЕ int64)
     • predict_X_Y (НЕ predict_target_X_Y) — ошибка повторяется каждый раз


═══════════════════════════════════════════════════════════════════════════════════
 АРТЕФАКТЫ (Google Drive: data_fusion/)
═══════════════════════════════════════════════════════════════════════════════════

  Шаг 0.5:
  • optuna_best_params.json — 41 набор гиперпараметров
  • xgb_best_features_optuna.json — 41 список фичей (имена строками)

  Шаг 1:
  • xgb_oof_optuna.npy — OOF матрица (750k, 41)

  Шаг 2:
  • xgb_test_optuna.npy — L1 test предсказания (250k, 41)

  Шаг 3:
  • l2_test.npy — L2 XGB test предсказания (250k, 41)

  Шаг 4-5:
  • nn_l2_oof.npy — NN L2 OOF (750k, 41)
  • submission_EXP012b.parquet — финальный сабмит


═══════════════════════════════════════════════════════════════════════════════════
 TIMELINE (ДЛИТЕЛЬНОСТЬ)
═══════════════════════════════════════════════════════════════════════════════════

  Шаг 0:   Загрузка данных .......................... ~2 мин
  Шаг 0.5: Optuna (один раз) ....................... ~4-6 часов (T4)
  Шаг 1:   L1 OOF 41 × 5-fold × Optuna iter ....... ~135 мин (T4)
  Шаг 2:   Full train 41 × Optuna iter × 1.2 ...... ~22 мин (T4)
  Шаг 3:   L2a XGB 41 × 5-fold × 100 iter ......... ~1 мин
  Шаг 4:   L2b NN 100 эпох full train .............. ~2 мин (T4) / ~40 сек (A100)
  Шаг 5:   Бленд + сабмит .......................... ~0.1 мин
                                                      ─────────
                                               ИТОГО: ~165 мин (~2.75 часа)
                                          (без Optuna, она уже посчитана)
