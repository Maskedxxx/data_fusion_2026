{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-014: 3-Model L1 Stacking + L2 XGB/NN Blend\n",
    "\n",
    "**LB: 0.8515** (рекорд)\n",
    "\n",
    "## Пайплайн:\n",
    "```\n",
    "Step 0.5: Per-target Optuna (XGB 200k/3f/30t + CB 200k/3f/30t + LGB 100k/3f/20t)\n",
    "Step 1:   L1 OOF 750k 5-fold (XGB GPU + CB GPU + LGB CPU) → 6 npy\n",
    "Step 2a:  L2 XGB Optuna per-target (15 trials) → OOF 0.8457\n",
    "Step 2b:  L2 NN v2 (LayerNorm, StandardScaler, 3 blocks) → OOF 0.8414\n",
    "Final:    Blend 60/40 XGB+NN → OOF 0.8479 → LB 0.8515\n",
    "```\n",
    "\n",
    "**Каждый шаг проверяет артефакты → если есть, загружает и пропускает.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup + Environment\n",
    "# ============================================================\n",
    "!pip install -q xgboost catboost lightgbm optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "log_msg = lambda msg: print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXP-014 FULL PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"XGBoost:  {xgb.__version__}\")\n",
    "print(f\"CatBoost: {cb.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"Optuna:   {optuna.__version__}\")\n",
    "\n",
    "# GPU check\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "# Quick GPU test\n",
    "dm = xgb.DMatrix(np.random.randn(1000, 10), label=np.random.randint(0, 2, 1000))\n",
    "xgb.train({'device': 'cuda', 'objective': 'binary:logistic', 'verbosity': 0}, dm, num_boost_round=10)\n",
    "print(\"XGBoost GPU: OK\")\n",
    "\n",
    "pool = cb.Pool(np.random.randn(1000, 10), label=np.random.randint(0, 2, 1000))\n",
    "cb.CatBoostClassifier(iterations=10, task_type='GPU', verbose=0, bootstrap_type='Poisson', subsample=0.8).fit(pool)\n",
    "print(\"CatBoost GPU: OK\")\n",
    "del dm, pool; gc.collect()\n",
    "print(\"\\nSETUP COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Config + Drive + Artifact Paths\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "if not os.path.isdir('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print('Drive already mounted')\n",
    "\n",
    "# --- Paths ---\n",
    "DATA = '/content/drive/MyDrive/data_fusion'\n",
    "ART_XGB  = f'{DATA}/artifacts/step05_xgb'\n",
    "ART_CB   = f'{DATA}/artifacts/step05_cb'\n",
    "ART_LGB  = f'{DATA}/artifacts/step05_lgb'\n",
    "ART_L1   = f'{DATA}/artifacts/l1_oof'\n",
    "ART_L2   = f'{DATA}/artifacts/l2_stacking'\n",
    "\n",
    "for d in [ART_XGB, ART_CB, ART_LGB, ART_L1, ART_L2]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# --- Config ---\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS_L1 = 5\n",
    "N_FOLDS_L2 = 5\n",
    "\n",
    "# Step 0.5 config\n",
    "OPTUNA_XGB = {'sample': 200_000, 'folds': 3, 'trials': 30, 'fs_thresh': 0.85}\n",
    "OPTUNA_CB  = {'sample': 200_000, 'folds': 3, 'trials': 30, 'fs_thresh': 0.85}\n",
    "OPTUNA_LGB = {'sample': 100_000, 'folds': 3, 'trials': 20, 'fs_thresh': 0.85}\n",
    "\n",
    "# --- Check artifacts ---\n",
    "def check_file(path):\n",
    "    exists = os.path.exists(path)\n",
    "    size = os.path.getsize(path) / 1e6 if exists else 0\n",
    "    return exists, size\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARTIFACT STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "artifacts = {\n",
    "    'Step 0.5 XGB params':  f'{ART_XGB}/xgb_best_params.json',\n",
    "    'Step 0.5 XGB feats':   f'{ART_XGB}/xgb_best_features.json',\n",
    "    'Step 0.5 CB params':   f'{ART_CB}/cb_best_params.json',\n",
    "    'Step 0.5 CB feats':    f'{ART_CB}/cb_best_features.json',\n",
    "    'Step 0.5 LGB params':  f'{ART_LGB}/lgb_best_params.json',\n",
    "    'Step 0.5 LGB feats':   f'{ART_LGB}/lgb_best_features.json',\n",
    "    'L1 OOF XGB':           f'{ART_L1}/oof_xgb.npy',\n",
    "    'L1 OOF CB':            f'{ART_L1}/oof_cb.npy',\n",
    "    'L1 OOF LGB':           f'{ART_L1}/oof_lgb.npy',\n",
    "    'L1 Test XGB':          f'{ART_L1}/test_xgb.npy',\n",
    "    'L1 Test CB':           f'{ART_L1}/test_cb.npy',\n",
    "    'L1 Test LGB':          f'{ART_L1}/test_lgb.npy',\n",
    "    'L2 XGB params':        f'{ART_L2}/l2_xgb_best_params.json',\n",
    "    'L2 OOF XGB':           f'{ART_L2}/oof_l2_xgb.npy',\n",
    "    'L2 Test XGB':          f'{ART_L2}/test_l2_xgb.npy',\n",
    "    'L2 OOF NN':            f'{ART_L2}/oof_l2_nn_v2.npy',\n",
    "    'L2 Test NN':           f'{ART_L2}/test_l2_nn_v2.npy',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for name, path in artifacts.items():\n",
    "    exists, size = check_file(path)\n",
    "    status = f'OK ({size:.1f} MB)' if exists else 'MISSING'\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "    print(f\"  {name:<22} {status}\")\n",
    "\n",
    "# Data files\n",
    "print(f\"\\nDATA FILES:\")\n",
    "for f in ['train_target.parquet', 'train_main_features.parquet',\n",
    "          'train_extra_features.parquet', 'test_main_features.parquet',\n",
    "          'test_extra_features.parquet']:\n",
    "    exists, size = check_file(f'{DATA}/{f}')\n",
    "    print(f\"  {f:<35} {'OK' if exists else 'MISSING'} ({size:.0f} MB)\")\n",
    "\n",
    "if all_ok:\n",
    "    print(f\"\\n>>> ВСЕ АРТЕФАКТЫ НАЙДЕНЫ — можно сразу прыгать на L2 или Blend <<<\")\n",
    "else:\n",
    "    print(f\"\\n>>> Есть MISSING артефакты — нужно запустить соответствующие шаги <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0.5: Per-target Optuna (XGB + CB + LGB)\n",
    "\n",
    "**Пропускается если артефакты уже есть.**\n",
    "\n",
    "- XGB: 200k sample, 3-fold, 30 trials (~200 min)\n",
    "- CB: 200k sample, 3-fold, 30 trials (~680 min)\n",
    "- LGB: 100k sample, 3-fold, 20 trials (~216 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load data for Step 0.5 / L1 (if needed)\n",
    "# ============================================================\n",
    "\n",
    "# Проверяем нужно ли грузить данные\n",
    "need_step05 = not all([\n",
    "    os.path.exists(f'{ART_XGB}/xgb_best_params.json'),\n",
    "    os.path.exists(f'{ART_CB}/cb_best_params.json'),\n",
    "    os.path.exists(f'{ART_LGB}/lgb_best_params.json'),\n",
    "])\n",
    "need_l1 = not os.path.exists(f'{ART_L1}/oof_xgb.npy')\n",
    "\n",
    "if need_step05 or need_l1:\n",
    "    print(\"Загрузка данных (нужны для Step 0.5 или L1)...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Таргеты\n",
    "    train_target = pd.read_parquet(f'{DATA}/train_target.parquet')\n",
    "    target_cols = [c for c in train_target.columns if c.startswith('target_')]\n",
    "    train_ids = train_target['customer_id'].values\n",
    "    print(f\"  Targets: {train_target.shape}\")\n",
    "\n",
    "    # Main features (float32 для RAM)\n",
    "    train_main = pd.read_parquet(f'{DATA}/train_main_features.parquet')\n",
    "    num_cols = train_main.select_dtypes(include=[np.number]).columns.drop('customer_id', errors='ignore')\n",
    "    train_main[num_cols] = train_main[num_cols].astype(np.float32)\n",
    "    print(f\"  Main: {train_main.shape}, {train_main.memory_usage(deep=True).sum()/1e9:.1f} GB\")\n",
    "\n",
    "    # Extra features (float32 для RAM)\n",
    "    train_extra = pd.read_parquet(f'{DATA}/train_extra_features.parquet')\n",
    "    num_cols_e = train_extra.select_dtypes(include=[np.number]).columns.drop('customer_id', errors='ignore')\n",
    "    train_extra[num_cols_e] = train_extra[num_cols_e].astype(np.float32)\n",
    "    print(f\"  Extra: {train_extra.shape}, {train_extra.memory_usage(deep=True).sum()/1e9:.1f} GB\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Merge\n",
    "    X_full = train_main.merge(train_extra, on='customer_id', how='inner')\n",
    "    del train_main, train_extra; gc.collect()\n",
    "\n",
    "    # Выравнивание по customer_id\n",
    "    X_full = X_full.set_index('customer_id').loc[train_ids].reset_index()\n",
    "    y_full = train_target.set_index('customer_id').loc[train_ids].reset_index()\n",
    "    del train_target; gc.collect()\n",
    "\n",
    "    # Разделяем\n",
    "    feature_names = [c for c in X_full.columns if c != 'customer_id']\n",
    "    y_sample_full = y_full[target_cols]\n",
    "    X_sample_full = X_full[feature_names]\n",
    "    train_customer_ids = X_full['customer_id'].values\n",
    "\n",
    "    print(f\"\\n  X_full: {X_sample_full.shape}\")\n",
    "    print(f\"  y_full: {y_sample_full.shape}\")\n",
    "    print(f\"  Loaded in {time.time()-t0:.0f}s\")\n",
    "else:\n",
    "    # Грузим только таргеты (нужны для L2)\n",
    "    train_target = pd.read_parquet(f'{DATA}/train_target.parquet')\n",
    "    target_cols = [c for c in train_target.columns if c.startswith('target_')]\n",
    "    train_ids = train_target['customer_id'].values\n",
    "    y_train_arr = train_target[target_cols].values.astype(np.int8)\n",
    "    del train_target; gc.collect()\n",
    "    print(f\"Step 0.5 + L1 артефакты найдены, данные не грузим.\")\n",
    "    print(f\"Загружены только таргеты: {y_train_arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Step 0.5 — Feature Selection + Optuna functions\n",
    "# ============================================================\n",
    "\n",
    "def select_features_xgb(X_data, y_target, threshold=0.85):\n",
    "    \"\"\"FS через XGBoost gain importance (GPU).\"\"\"\n",
    "    dm = xgb.DMatrix(X_data, label=y_target, nthread=-1)\n",
    "    params = {'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "              'device': 'cuda', 'max_depth': 6, 'learning_rate': 0.1, 'verbosity': 0}\n",
    "    model = xgb.train(params, dm, num_boost_round=200)\n",
    "    importance = model.get_score(importance_type='total_gain')\n",
    "    if not importance:\n",
    "        return list(X_data.columns)\n",
    "    sorted_feats = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in sorted_feats)\n",
    "    selected, cumulative = [], 0.0\n",
    "    for feat, gain in sorted_feats:\n",
    "        selected.append(feat)\n",
    "        cumulative += gain / total_gain\n",
    "        if cumulative >= threshold:\n",
    "            break\n",
    "    del model, dm; gc.collect()\n",
    "    return selected\n",
    "\n",
    "\n",
    "def select_features_cb(X_data, y_target, threshold=0.85):\n",
    "    \"\"\"FS через CatBoost PredictionValuesChange (GPU).\"\"\"\n",
    "    model = cb.CatBoostClassifier(\n",
    "        iterations=200, depth=6, learning_rate=0.1,\n",
    "        loss_function='Logloss', bootstrap_type='Poisson', subsample=0.8,\n",
    "        task_type='GPU', random_seed=RANDOM_SEED, verbose=0)\n",
    "    model.fit(cb.Pool(X_data, label=y_target))\n",
    "    imp = model.get_feature_importance(type=cb.EFstrType.PredictionValuesChange)\n",
    "    feat_names = X_data.columns.tolist()\n",
    "    pairs = sorted(zip(feat_names, imp), key=lambda x: x[1], reverse=True)\n",
    "    total_imp = sum(v for _, v in pairs)\n",
    "    if total_imp == 0:\n",
    "        return feat_names\n",
    "    selected, cum = [], 0.0\n",
    "    for name, importance in pairs:\n",
    "        cum += importance\n",
    "        selected.append(name)\n",
    "        if cum / total_imp >= threshold:\n",
    "            break\n",
    "    if len(selected) < 10:\n",
    "        selected = [n for n, _ in pairs[:10]]\n",
    "    del model; gc.collect()\n",
    "    return selected\n",
    "\n",
    "\n",
    "def select_features_lgb(X_data, y_target, threshold=0.85):\n",
    "    \"\"\"FS через LightGBM gain importance (CPU).\"\"\"\n",
    "    dtrain = lgb.Dataset(X_data, label=y_target)\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary', 'metric': 'auc', 'verbosity': -1,\n",
    "         'n_jobs': -1, 'num_leaves': 63, 'learning_rate': 0.1},\n",
    "        dtrain, num_boost_round=200,\n",
    "        valid_sets=[dtrain], valid_names=['train'],\n",
    "        callbacks=[lgb.log_evaluation(0)])\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_names = model.feature_name()\n",
    "    pairs = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)\n",
    "    total_gain = sum(v for _, v in pairs)\n",
    "    if total_gain == 0:\n",
    "        return list(X_data.columns)\n",
    "    selected, cumulative = [], 0.0\n",
    "    for feat, gain in pairs:\n",
    "        selected.append(feat)\n",
    "        cumulative += gain / total_gain\n",
    "        if cumulative >= threshold:\n",
    "            break\n",
    "    del model, dtrain; gc.collect()\n",
    "    return selected\n",
    "\n",
    "print(\"FS functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Step 0.5 — XGBoost Optuna (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "if os.path.exists(f'{ART_XGB}/xgb_best_params.json'):\n",
    "    print(\">>> XGB Optuna: SKIP (артефакты найдены) <<<\")\n",
    "    with open(f'{ART_XGB}/xgb_best_params.json') as f:\n",
    "        xgb_params = json.load(f)\n",
    "    with open(f'{ART_XGB}/xgb_best_features.json') as f:\n",
    "        xgb_features = json.load(f)\n",
    "    print(f\"  Loaded: {len(xgb_params)} targets\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Step 0.5: XGBoost Optuna\")\n",
    "    cfg = OPTUNA_XGB\n",
    "    print(f\"Sample: {cfg['sample']:,}, Folds: {cfg['folds']}, Trials: {cfg['trials']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Sample data\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    idx = np.random.choice(len(X_sample_full), cfg['sample'], replace=False)\n",
    "    X_s = X_sample_full.iloc[idx].reset_index(drop=True)\n",
    "    y_s = y_sample_full.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    xgb_params, xgb_features = {}, {}\n",
    "    # Checkpoint\n",
    "    cp_path = f'{ART_XGB}/checkpoint_xgb.json'\n",
    "    if os.path.exists(cp_path):\n",
    "        with open(cp_path) as f:\n",
    "            cp = json.load(f)\n",
    "        xgb_params = cp.get('best_params', {})\n",
    "        xgb_features = cp.get('best_features', {})\n",
    "        print(f\"  Checkpoint: {len(xgb_params)} targets done\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        if tcol in xgb_params:\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP\"); continue\n",
    "        y_t = y_s[tcol]\n",
    "        n_pos = int(y_t.sum())\n",
    "        if n_pos < cfg['folds'] * 2:\n",
    "            xgb_params[tcol] = {}; xgb_features[tcol] = list(X_s.columns)\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP (pos={n_pos})\"); continue\n",
    "\n",
    "        selected = select_features_xgb(X_s, y_t, threshold=cfg['fs_thresh'])\n",
    "        X_sel = X_s[selected]\n",
    "        log_msg(f\"[{i+1}/41] {tcol}: pos={y_t.mean():.4f}, FS: {X_s.shape[1]}>{len(selected)}\")\n",
    "\n",
    "        dm_full = xgb.DMatrix(X_sel, label=y_t, nthread=-1)\n",
    "        skf = StratifiedKFold(n_splits=cfg['folds'], shuffle=True, random_state=RANDOM_SEED)\n",
    "        folds = list(skf.split(X_sel, y_t))\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "                'device': 'cuda', 'n_jobs': 1, 'verbosity': 0,\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 25.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "                'max_bin': trial.suggest_int('max_bin', 128, 512, step=64),\n",
    "                'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "            }\n",
    "            pruning_cb = XGBoostPruningCallback(trial, 'test-auc')\n",
    "            try:\n",
    "                cv_res = xgb.cv(params, dm_full, num_boost_round=1000, folds=folds,\n",
    "                                early_stopping_rounds=50, callbacks=[pruning_cb], verbose_eval=False)\n",
    "                return cv_res['test-auc-mean'].max()\n",
    "            except optuna.exceptions.TrialPruned:\n",
    "                raise\n",
    "            except:\n",
    "                return 0.5\n",
    "\n",
    "        t0 = time.time()\n",
    "        study = optuna.create_study(direction='maximize',\n",
    "                                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "        study.optimize(objective, n_trials=cfg['trials'], show_progress_bar=True)\n",
    "\n",
    "        xgb_params[tcol] = study.best_params\n",
    "        xgb_features[tcol] = selected\n",
    "        log_msg(f\"  AUC={study.best_value:.4f}, time={time.time()-t0:.0f}s\")\n",
    "\n",
    "        del dm_full, study; gc.collect()\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            with open(cp_path, 'w') as f:\n",
    "                json.dump({'best_params': xgb_params, 'best_features': xgb_features}, f, indent=2)\n",
    "            log_msg(f\"  Checkpoint: {len(xgb_params)}/41\")\n",
    "\n",
    "    with open(f'{ART_XGB}/xgb_best_params.json', 'w') as f:\n",
    "        json.dump(xgb_params, f, indent=2)\n",
    "    with open(f'{ART_XGB}/xgb_best_features.json', 'w') as f:\n",
    "        json.dump(xgb_features, f, indent=2)\n",
    "    log_msg(f\"XGB Optuna DONE: {(time.time()-t_start)/60:.0f} min\")\n",
    "    del X_s, y_s; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Step 0.5 — CatBoost Optuna (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "\n",
    "if os.path.exists(f'{ART_CB}/cb_best_params.json'):\n",
    "    print(\">>> CB Optuna: SKIP (артефакты найдены) <<<\")\n",
    "    with open(f'{ART_CB}/cb_best_params.json') as f:\n",
    "        cb_params = json.load(f)\n",
    "    with open(f'{ART_CB}/cb_best_features.json') as f:\n",
    "        cb_features = json.load(f)\n",
    "    print(f\"  Loaded: {len(cb_params)} targets\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Step 0.5: CatBoost Optuna\")\n",
    "    cfg = OPTUNA_CB\n",
    "    print(f\"Sample: {cfg['sample']:,}, Folds: {cfg['folds']}, Trials: {cfg['trials']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    idx = np.random.choice(len(X_sample_full), cfg['sample'], replace=False)\n",
    "    X_s = X_sample_full.iloc[idx].reset_index(drop=True)\n",
    "    y_s = y_sample_full.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    cb_params, cb_features = {}, {}\n",
    "    cp_path = f'{ART_CB}/checkpoint_cb.json'\n",
    "    if os.path.exists(cp_path):\n",
    "        with open(cp_path) as f:\n",
    "            cp = json.load(f)\n",
    "        cb_params = cp.get('best_params', {})\n",
    "        cb_features = cp.get('best_features', {})\n",
    "        print(f\"  Checkpoint: {len(cb_params)} targets done\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        if tcol in cb_params:\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP\"); continue\n",
    "        y_t = y_s[tcol]\n",
    "        n_pos = int(y_t.sum())\n",
    "        if n_pos < cfg['folds'] * 2:\n",
    "            cb_params[tcol] = {}; cb_features[tcol] = list(X_s.columns)\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP (pos={n_pos})\"); continue\n",
    "\n",
    "        selected = select_features_cb(X_s, y_t.values, threshold=cfg['fs_thresh'])\n",
    "        X_sel = X_s[selected]\n",
    "        log_msg(f\"[{i+1}/41] {tcol}: pos={y_t.mean():.4f}, FS: {X_s.shape[1]}>{len(selected)}\")\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=cfg['folds'], shuffle=True, random_state=RANDOM_SEED)\n",
    "        folds = list(skf.split(X_sel, y_t))\n",
    "        pools_tr = [cb.Pool(X_sel.iloc[tr], label=y_t.iloc[tr]) for tr, _ in folds]\n",
    "        pools_val = [cb.Pool(X_sel.iloc[vl], label=y_t.iloc[vl]) for _, vl in folds]\n",
    "        val_labels = [y_t.iloc[vl].values for _, vl in folds]\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'iterations': 2000, 'loss_function': 'Logloss',\n",
    "                'task_type': 'GPU', 'random_seed': RANDOM_SEED, 'verbose': 0,\n",
    "                'use_best_model': True, 'bootstrap_type': 'Poisson',\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 100, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0.0, 20.0),\n",
    "                'border_count': trial.suggest_int('border_count', 32, 254),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            }\n",
    "            aucs = []\n",
    "            for fi in range(cfg['folds']):\n",
    "                model = cb.CatBoostClassifier(**params)\n",
    "                model.fit(pools_tr[fi], eval_set=pools_val[fi], early_stopping_rounds=50)\n",
    "                pred = model.predict_proba(pools_val[fi])[:, 1]\n",
    "                aucs.append(roc_auc_score(val_labels[fi], pred))\n",
    "                del model\n",
    "            gc.collect()\n",
    "            return np.mean(aucs)\n",
    "\n",
    "        t0 = time.time()\n",
    "        study = optuna.create_study(direction='maximize',\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "        study.optimize(objective, n_trials=cfg['trials'], show_progress_bar=True)\n",
    "\n",
    "        cb_params[tcol] = study.best_params\n",
    "        cb_features[tcol] = selected\n",
    "        log_msg(f\"  AUC={study.best_value:.4f}, time={time.time()-t0:.0f}s\")\n",
    "\n",
    "        del pools_tr, pools_val, study; gc.collect()\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            with open(cp_path, 'w') as f:\n",
    "                json.dump({'best_params': cb_params, 'best_features': cb_features}, f, indent=2)\n",
    "            log_msg(f\"  Checkpoint: {len(cb_params)}/41\")\n",
    "\n",
    "    with open(f'{ART_CB}/cb_best_params.json', 'w') as f:\n",
    "        json.dump(cb_params, f, indent=2)\n",
    "    with open(f'{ART_CB}/cb_best_features.json', 'w') as f:\n",
    "        json.dump(cb_features, f, indent=2)\n",
    "    log_msg(f\"CB Optuna DONE: {(time.time()-t_start)/60:.0f} min\")\n",
    "    del X_s, y_s; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Step 0.5 — LightGBM Optuna (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "\n",
    "if os.path.exists(f'{ART_LGB}/lgb_best_params.json'):\n",
    "    print(\">>> LGB Optuna: SKIP (артефакты найдены) <<<\")\n",
    "    with open(f'{ART_LGB}/lgb_best_params.json') as f:\n",
    "        lgb_params = json.load(f)\n",
    "    with open(f'{ART_LGB}/lgb_best_features.json') as f:\n",
    "        lgb_features = json.load(f)\n",
    "    print(f\"  Loaded: {len(lgb_params)} targets\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Step 0.5: LightGBM Optuna\")\n",
    "    cfg = OPTUNA_LGB\n",
    "    print(f\"Sample: {cfg['sample']:,}, Folds: {cfg['folds']}, Trials: {cfg['trials']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    idx = np.random.choice(len(X_sample_full), cfg['sample'], replace=False)\n",
    "    X_s = X_sample_full.iloc[idx].reset_index(drop=True)\n",
    "    y_s = y_sample_full.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    lgb_params, lgb_features = {}, {}\n",
    "    cp_path = f'{ART_LGB}/checkpoint_lgb.json'\n",
    "    if os.path.exists(cp_path):\n",
    "        with open(cp_path) as f:\n",
    "            cp = json.load(f)\n",
    "        lgb_params = cp.get('best_params', {})\n",
    "        lgb_features = cp.get('best_features', {})\n",
    "        print(f\"  Checkpoint: {len(lgb_params)} targets done\")\n",
    "\n",
    "    from optuna.integration import LightGBMPruningCallback\n",
    "    import warnings as w\n",
    "    w.filterwarnings('ignore', message='The reported value is ignored')\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        if tcol in lgb_params:\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP\"); continue\n",
    "        y_t = y_s[tcol]\n",
    "        n_pos = int(y_t.sum())\n",
    "        if n_pos < cfg['folds'] * 2:\n",
    "            lgb_params[tcol] = {}; lgb_features[tcol] = list(X_s.columns)\n",
    "            log_msg(f\"[{i+1}/41] {tcol}: SKIP (pos={n_pos})\"); continue\n",
    "\n",
    "        selected = select_features_lgb(X_s, y_t, threshold=cfg['fs_thresh'])\n",
    "        X_sel = X_s[selected]\n",
    "        log_msg(f\"[{i+1}/41] {tcol}: pos={y_t.mean():.4f}, FS: {X_s.shape[1]}>{len(selected)}\")\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=cfg['folds'], shuffle=True, random_state=RANDOM_SEED)\n",
    "        folds = list(skf.split(X_sel, y_t))\n",
    "        X_np = X_sel.values; y_np = y_t.values; feat_names = selected\n",
    "        ds_tr = [lgb.Dataset(X_np[tr], label=y_np[tr], feature_name=feat_names, free_raw_data=False) for tr, _ in folds]\n",
    "        ds_val = [lgb.Dataset(X_np[vl], label=y_np[vl], reference=ds_tr[j], feature_name=feat_names, free_raw_data=False) for j, (_, vl) in enumerate(folds)]\n",
    "        val_idxs = [vl for _, vl in folds]\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'n_jobs': -1,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 16, 255),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 2500),\n",
    "                'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 100.0, log=True),\n",
    "                'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 100.0, log=True),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.05, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 15.0),\n",
    "                'path_smooth': trial.suggest_float('path_smooth', 0.0, 10.0),\n",
    "                'max_bin': trial.suggest_int('max_bin', 63, 511),\n",
    "            }\n",
    "            aucs = []\n",
    "            for fi in range(cfg['folds']):\n",
    "                pruning_cb = LightGBMPruningCallback(trial, 'auc', valid_name='val')\n",
    "                model = lgb.train(params, ds_tr[fi], num_boost_round=1000,\n",
    "                                  valid_sets=[ds_val[fi]], valid_names=['val'],\n",
    "                                  callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0), pruning_cb])\n",
    "                pred = model.predict(X_np[val_idxs[fi]])\n",
    "                aucs.append(roc_auc_score(y_np[val_idxs[fi]], pred))\n",
    "                del model\n",
    "            gc.collect()\n",
    "            return np.mean(aucs)\n",
    "\n",
    "        t0 = time.time()\n",
    "        study = optuna.create_study(direction='maximize',\n",
    "                                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "        study.optimize(objective, n_trials=cfg['trials'], show_progress_bar=True)\n",
    "\n",
    "        lgb_params[tcol] = study.best_params\n",
    "        lgb_features[tcol] = selected\n",
    "        log_msg(f\"  AUC={study.best_value:.4f}, time={time.time()-t0:.0f}s\")\n",
    "\n",
    "        del ds_tr, ds_val, study; gc.collect()\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            with open(cp_path, 'w') as f:\n",
    "                json.dump({'best_params': lgb_params, 'best_features': lgb_features}, f, indent=2)\n",
    "            log_msg(f\"  Checkpoint: {len(lgb_params)}/41\")\n",
    "\n",
    "    with open(f'{ART_LGB}/lgb_best_params.json', 'w') as f:\n",
    "        json.dump(lgb_params, f, indent=2)\n",
    "    with open(f'{ART_LGB}/lgb_best_features.json', 'w') as f:\n",
    "        json.dump(lgb_features, f, indent=2)\n",
    "    log_msg(f\"LGB Optuna DONE: {(time.time()-t_start)/60:.0f} min\")\n",
    "    del X_s, y_s; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: L1 OOF (750k, 5-fold, 3 models)\n",
    "\n",
    "**~400 min. Пропускается если `oof_xgb.npy` уже есть.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: L1 OOF functions\n",
    "# ============================================================\n",
    "\n",
    "def l1_oof_xgb(X, y, X_te, params_dict, features, target_col,\n",
    "               n_folds=N_FOLDS_L1, seed=RANDOM_SEED):\n",
    "    \"\"\"L1 OOF XGBoost GPU. Returns: oof (n,), test (m,), auc.\"\"\"\n",
    "    feats = features[target_col]\n",
    "    p = params_dict[target_col]\n",
    "    if not p:\n",
    "        return np.zeros(len(X)), np.zeros(len(X_te)), 0.0\n",
    "    X_sel = X[feats].values; X_te_sel = X_te[feats].values; y_arr = y.values\n",
    "    oof = np.zeros(len(X)); test_preds = np.zeros(len(X_te))\n",
    "    dm_te = xgb.DMatrix(X_te_sel)\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_sel, y_arr)):\n",
    "        xgb_p = {'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "                 'device': 'cuda', 'n_jobs': 1, 'verbosity': 0}\n",
    "        for k, v in p.items():\n",
    "            if k not in ('best_auc', 'default_auc', 'n_rounds'):\n",
    "                xgb_p[k] = v\n",
    "        dm_tr = xgb.DMatrix(X_sel[tr_idx], label=y_arr[tr_idx])\n",
    "        dm_val = xgb.DMatrix(X_sel[val_idx], label=y_arr[val_idx])\n",
    "        model = xgb.train(xgb_p, dm_tr, num_boost_round=2000,\n",
    "                          evals=[(dm_val, 'val')], verbose_eval=0, early_stopping_rounds=50)\n",
    "        oof[val_idx] = model.predict(dm_val)\n",
    "        test_preds += model.predict(dm_te) / n_folds\n",
    "        del model, dm_tr, dm_val\n",
    "    del dm_te; gc.collect()\n",
    "    return oof, test_preds, roc_auc_score(y_arr, oof)\n",
    "\n",
    "\n",
    "def l1_oof_cb(X, y, X_te, params_dict, features, target_col,\n",
    "              n_folds=N_FOLDS_L1, seed=RANDOM_SEED):\n",
    "    \"\"\"L1 OOF CatBoost GPU. Returns: oof (n,), test (m,), auc.\"\"\"\n",
    "    feats = features[target_col]\n",
    "    p = params_dict[target_col]\n",
    "    if not p:\n",
    "        return np.zeros(len(X)), np.zeros(len(X_te)), 0.0\n",
    "    X_sel = X[feats].values; X_te_sel = X_te[feats].values; y_arr = y.values\n",
    "    oof = np.zeros(len(X)); test_preds = np.zeros(len(X_te))\n",
    "    pool_te = cb.Pool(X_te_sel)\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_sel, y_arr)):\n",
    "        cb_p = {'iterations': 2000, 'loss_function': 'Logloss', 'task_type': 'GPU',\n",
    "                'random_seed': seed, 'verbose': 0, 'use_best_model': True,\n",
    "                'bootstrap_type': 'Poisson'}\n",
    "        for k, v in p.items():\n",
    "            if k not in ('best_auc', 'default_auc'):\n",
    "                cb_p[k] = v\n",
    "        pool_tr = cb.Pool(X_sel[tr_idx], label=y_arr[tr_idx])\n",
    "        pool_val = cb.Pool(X_sel[val_idx], label=y_arr[val_idx])\n",
    "        model = cb.CatBoostClassifier(**cb_p)\n",
    "        model.fit(pool_tr, eval_set=pool_val, early_stopping_rounds=50)\n",
    "        oof[val_idx] = model.predict_proba(pool_val)[:, 1]\n",
    "        test_preds += model.predict_proba(pool_te)[:, 1] / n_folds\n",
    "        del model, pool_tr, pool_val\n",
    "    del pool_te; gc.collect()\n",
    "    return oof, test_preds, roc_auc_score(y_arr, oof)\n",
    "\n",
    "\n",
    "def l1_oof_lgb(X, y, X_te, params_dict, features, target_col,\n",
    "               n_folds=N_FOLDS_L1, seed=RANDOM_SEED):\n",
    "    \"\"\"L1 OOF LightGBM CPU. Returns: oof (n,), test (m,), auc.\"\"\"\n",
    "    feats = features[target_col]\n",
    "    p = params_dict[target_col]\n",
    "    if not p:\n",
    "        return np.zeros(len(X)), np.zeros(len(X_te)), 0.0\n",
    "    X_sel = X[feats].values; X_te_sel = X_te[feats].values; y_arr = y.values\n",
    "    oof = np.zeros(len(X)); test_preds = np.zeros(len(X_te))\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_sel, y_arr)):\n",
    "        lgb_p = {'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'n_jobs': -1}\n",
    "        for k, v in p.items():\n",
    "            if k not in ('best_auc', 'default_auc'):\n",
    "                lgb_p[k] = v\n",
    "        dtrain = lgb.Dataset(X_sel[tr_idx], label=y_arr[tr_idx], feature_name=feats, free_raw_data=False)\n",
    "        dval = lgb.Dataset(X_sel[val_idx], label=y_arr[val_idx], reference=dtrain,\n",
    "                           feature_name=feats, free_raw_data=False)\n",
    "        model = lgb.train(lgb_p, dtrain, num_boost_round=2000, valid_sets=[dval], valid_names=['val'],\n",
    "                          callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "        oof[val_idx] = model.predict(X_sel[val_idx])\n",
    "        test_preds += model.predict(X_te_sel) / n_folds\n",
    "        del model, dtrain, dval\n",
    "    gc.collect()\n",
    "    return oof, test_preds, roc_auc_score(y_arr, oof)\n",
    "\n",
    "print(\"L1 OOF functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: L1 OOF Main Loop (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "\n",
    "if os.path.exists(f'{ART_L1}/oof_xgb.npy'):\n",
    "    print(\">>> L1 OOF: SKIP (артефакты найдены) <<<\")\n",
    "    oof_xgb = np.load(f'{ART_L1}/oof_xgb.npy')\n",
    "    oof_cb  = np.load(f'{ART_L1}/oof_cb.npy')\n",
    "    oof_lgb = np.load(f'{ART_L1}/oof_lgb.npy')\n",
    "    test_xgb = np.load(f'{ART_L1}/test_xgb.npy')\n",
    "    test_cb  = np.load(f'{ART_L1}/test_cb.npy')\n",
    "    test_lgb = np.load(f'{ART_L1}/test_lgb.npy')\n",
    "    print(f\"  OOF: {oof_xgb.shape}, Test: {test_xgb.shape}\")\n",
    "    # Load targets if not loaded\n",
    "    if 'y_train_arr' not in dir():\n",
    "        target = pd.read_parquet(f'{DATA}/train_target.parquet')\n",
    "        target_cols = [c for c in target.columns if c.startswith('target_')]\n",
    "        y_train_arr = target[target_cols].values.astype(np.int8)\n",
    "        train_ids = target['customer_id'].values\n",
    "        del target; gc.collect()\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"L1 OOF: 750k, 5-fold, 3 models\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load test\n",
    "    t0 = time.time()\n",
    "    main_te = pd.read_parquet(f'{DATA}/test_main_features.parquet')\n",
    "    num_cols = main_te.select_dtypes(include=[np.number]).columns.drop('customer_id', errors='ignore')\n",
    "    main_te[num_cols] = main_te[num_cols].astype(np.float32)\n",
    "    extra_te = pd.read_parquet(f'{DATA}/test_extra_features.parquet')\n",
    "    num_cols_e = extra_te.select_dtypes(include=[np.number]).columns.drop('customer_id', errors='ignore')\n",
    "    extra_te[num_cols_e] = extra_te[num_cols_e].astype(np.float32)\n",
    "    X_test = main_te.merge(extra_te, on='customer_id', how='inner')\n",
    "    del main_te, extra_te; gc.collect()\n",
    "    test_customer_ids = X_test['customer_id'].values\n",
    "    X_test = X_test.drop(columns=['customer_id'])\n",
    "    print(f\"Test loaded: {X_test.shape}, {time.time()-t0:.0f}s\")\n",
    "\n",
    "    n_train = len(X_sample_full)\n",
    "    n_test = len(X_test)\n",
    "    oof_xgb = np.zeros((n_train, 41), dtype=np.float32)\n",
    "    oof_cb = np.zeros((n_train, 41), dtype=np.float32)\n",
    "    oof_lgb = np.zeros((n_train, 41), dtype=np.float32)\n",
    "    test_xgb = np.zeros((n_test, 41), dtype=np.float32)\n",
    "    test_cb = np.zeros((n_test, 41), dtype=np.float32)\n",
    "    test_lgb = np.zeros((n_test, 41), dtype=np.float32)\n",
    "    results_log = []\n",
    "\n",
    "    # Checkpoint\n",
    "    start_idx = 0\n",
    "    cp_path = f'{ART_L1}/checkpoint_l1.json'\n",
    "    if os.path.exists(cp_path):\n",
    "        with open(cp_path) as f: cp = json.load(f)\n",
    "        start_idx = cp['last_target_idx'] + 1\n",
    "        for name in ['oof_xgb', 'oof_cb', 'oof_lgb', 'test_xgb', 'test_cb', 'test_lgb']:\n",
    "            arr = np.load(f'{ART_L1}/{name}_partial.npy')\n",
    "            locals()[name][:] = arr\n",
    "        results_log = cp.get('results_log', [])\n",
    "        print(f\"Checkpoint: resume from {start_idx}/41\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        if i < start_idx:\n",
    "            continue\n",
    "        y_t = y_sample_full[tcol]\n",
    "        log_msg(f\"[{i+1}/41] {tcol}: pos={y_t.mean():.4f}\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        oof_xgb[:, i], test_xgb[:, i], auc_x = l1_oof_xgb(\n",
    "            X_sample_full, y_t, X_test, xgb_params, xgb_features, tcol)\n",
    "        t_x = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        oof_cb[:, i], test_cb[:, i], auc_c = l1_oof_cb(\n",
    "            X_sample_full, y_t, X_test, cb_params, cb_features, tcol)\n",
    "        t_c = time.time() - t0\n",
    "\n",
    "        t0 = time.time()\n",
    "        oof_lgb[:, i], test_lgb[:, i], auc_l = l1_oof_lgb(\n",
    "            X_sample_full, y_t, X_test, lgb_params, lgb_features, tcol)\n",
    "        t_l = time.time() - t0\n",
    "\n",
    "        log_msg(f\"  XGB={auc_x:.4f}({t_x:.0f}s) CB={auc_c:.4f}({t_c:.0f}s) LGB={auc_l:.4f}({t_l:.0f}s)\")\n",
    "        results_log.append({'target': tcol, 'auc_xgb': round(auc_x, 4),\n",
    "                            'auc_cb': round(auc_c, 4), 'auc_lgb': round(auc_l, 4)})\n",
    "        gc.collect()\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            for name in ['oof_xgb', 'oof_cb', 'oof_lgb', 'test_xgb', 'test_cb', 'test_lgb']:\n",
    "                np.save(f'{ART_L1}/{name}_partial.npy', locals()[name])\n",
    "            with open(cp_path, 'w') as f:\n",
    "                json.dump({'last_target_idx': i, 'results_log': results_log}, f)\n",
    "            elapsed = (time.time() - t_start) / 60\n",
    "            done = i + 1 - start_idx\n",
    "            eta = elapsed / done * (41 - i - 1) if done > 0 else 0\n",
    "            log_msg(f\"  Checkpoint {i+1}/41, elapsed={elapsed:.0f}min, ETA={eta:.0f}min\")\n",
    "\n",
    "    # Save final\n",
    "    for name in ['oof_xgb', 'oof_cb', 'oof_lgb', 'test_xgb', 'test_cb', 'test_lgb']:\n",
    "        np.save(f'{ART_L1}/{name}.npy', locals()[name])\n",
    "    with open(f'{ART_L1}/results_log.json', 'w') as f:\n",
    "        json.dump(results_log, f, indent=2)\n",
    "\n",
    "    y_train_arr = y_sample_full.values.astype(np.int8)\n",
    "    elapsed = (time.time() - t_start) / 60\n",
    "    log_msg(f\"L1 OOF DONE: {elapsed:.0f} min\")\n",
    "    del X_test; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: L2 Stacking (XGB + NN Blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Build L2 matrix (123 OOF + 82 meta = 205 features)\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"Building L2 matrix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure targets loaded\n",
    "if 'y_train_arr' not in dir():\n",
    "    target = pd.read_parquet(f'{DATA}/train_target.parquet')\n",
    "    target_cols = [c for c in target.columns if c.startswith('target_')]\n",
    "    y_train_arr = target[target_cols].values.astype(np.int8)\n",
    "    train_ids = target['customer_id'].values\n",
    "    del target; gc.collect()\n",
    "\n",
    "# Ensure L1 loaded\n",
    "if 'oof_xgb' not in dir():\n",
    "    oof_xgb = np.load(f'{ART_L1}/oof_xgb.npy')\n",
    "    oof_cb  = np.load(f'{ART_L1}/oof_cb.npy')\n",
    "    oof_lgb = np.load(f'{ART_L1}/oof_lgb.npy')\n",
    "    test_xgb = np.load(f'{ART_L1}/test_xgb.npy')\n",
    "    test_cb  = np.load(f'{ART_L1}/test_cb.npy')\n",
    "    test_lgb = np.load(f'{ART_L1}/test_lgb.npy')\n",
    "\n",
    "# 123 OOF features\n",
    "X_l2_train = np.hstack([oof_xgb, oof_cb, oof_lgb])   # (750k, 123)\n",
    "X_l2_test  = np.hstack([test_xgb, test_cb, test_lgb]) # (250k, 123)\n",
    "\n",
    "# 82 meta features (mean + std across 3 models)\n",
    "oof_stack = np.stack([oof_xgb, oof_cb, oof_lgb], axis=0)\n",
    "test_stack = np.stack([test_xgb, test_cb, test_lgb], axis=0)\n",
    "X_l2_train = np.hstack([X_l2_train, oof_stack.mean(0), oof_stack.std(0)])\n",
    "X_l2_test  = np.hstack([X_l2_test, test_stack.mean(0), test_stack.std(0)])\n",
    "del oof_stack, test_stack; gc.collect()\n",
    "\n",
    "print(f\"X_l2_train: {X_l2_train.shape} ({X_l2_train.nbytes/1e6:.0f} MB)\")\n",
    "print(f\"X_l2_test:  {X_l2_test.shape}\")\n",
    "print(f\"Features: 123 OOF + 41 mean + 41 std = {X_l2_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: L2 XGB Optuna + OOF (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "N_TRIALS_L2 = 15\n",
    "\n",
    "if os.path.exists(f'{ART_L2}/oof_l2_xgb.npy'):\n",
    "    print(\">>> L2 XGB: SKIP (артефакты найдены) <<<\")\n",
    "    with open(f'{ART_L2}/l2_xgb_best_params.json') as f:\n",
    "        l2_best_params = json.load(f)\n",
    "    oof_l2_xgb = np.load(f'{ART_L2}/oof_l2_xgb.npy')\n",
    "    test_l2_xgb = np.load(f'{ART_L2}/test_l2_xgb.npy')\n",
    "    macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_xgb[:, i]) for i in range(41)])\n",
    "    print(f\"  OOF Macro AUC: {macro:.4f}\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"L2 XGB Optuna: {N_TRIALS_L2} trials, {N_FOLDS_L2}-fold\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def l2_objective(trial, X, y):\n",
    "        params = {\n",
    "            'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "            'device': 'cuda', 'n_jobs': 1, 'verbosity': 0,\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 50.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        }\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS_L2, shuffle=True, random_state=RANDOM_SEED)\n",
    "        aucs = []\n",
    "        for tr_idx, val_idx in skf.split(X, y):\n",
    "            dm_tr = xgb.DMatrix(X[tr_idx], label=y[tr_idx])\n",
    "            dm_val = xgb.DMatrix(X[val_idx], label=y[val_idx])\n",
    "            model = xgb.train(params, dm_tr, num_boost_round=500,\n",
    "                              evals=[(dm_val, 'val')], verbose_eval=0, early_stopping_rounds=30)\n",
    "            aucs.append(roc_auc_score(y[val_idx], model.predict(dm_val)))\n",
    "            del model, dm_tr, dm_val\n",
    "        gc.collect()\n",
    "        return np.mean(aucs)\n",
    "\n",
    "    # Per-target Optuna\n",
    "    l2_best_params = {}\n",
    "    t_start = time.time()\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        t0 = time.time()\n",
    "        y_col = y_train_arr[:, i]\n",
    "        study = optuna.create_study(direction='maximize',\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "        study.optimize(lambda trial: l2_objective(trial, X_l2_train, y_col),\n",
    "                       n_trials=N_TRIALS_L2, show_progress_bar=False)\n",
    "        bp = study.best_params; bp['best_auc'] = study.best_value\n",
    "        l2_best_params[tcol] = bp\n",
    "        log_msg(f\"[{i+1}/41] {tcol}: AUC={study.best_value:.4f}, depth={bp['max_depth']}, time={time.time()-t0:.0f}s\")\n",
    "        del study; gc.collect()\n",
    "\n",
    "    with open(f'{ART_L2}/l2_xgb_best_params.json', 'w') as f:\n",
    "        json.dump(l2_best_params, f, indent=2)\n",
    "    log_msg(f\"L2 Optuna DONE: {(time.time()-t_start)/60:.1f} min\")\n",
    "\n",
    "    # OOF + test predictions\n",
    "    print(f\"\\nL2 XGB OOF + Test...\")\n",
    "    oof_l2_xgb = np.zeros((len(X_l2_train), 41), dtype=np.float32)\n",
    "    test_l2_xgb = np.zeros((len(X_l2_test), 41), dtype=np.float32)\n",
    "\n",
    "    for i, tcol in enumerate(target_cols):\n",
    "        y_col = y_train_arr[:, i]\n",
    "        bp = l2_best_params[tcol]\n",
    "        xgb_p = {'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "                 'device': 'cuda', 'n_jobs': 1, 'verbosity': 0,\n",
    "                 'max_depth': bp['max_depth'], 'learning_rate': bp['learning_rate'],\n",
    "                 'colsample_bytree': bp['colsample_bytree'], 'subsample': bp['subsample'],\n",
    "                 'reg_lambda': bp['reg_lambda'], 'reg_alpha': bp['reg_alpha'],\n",
    "                 'min_child_weight': bp['min_child_weight']}\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS_L2, shuffle=True, random_state=RANDOM_SEED)\n",
    "        dm_te = xgb.DMatrix(X_l2_test)\n",
    "        for tr_idx, val_idx in skf.split(X_l2_train, y_col):\n",
    "            dm_tr = xgb.DMatrix(X_l2_train[tr_idx], label=y_col[tr_idx])\n",
    "            dm_val = xgb.DMatrix(X_l2_train[val_idx], label=y_col[val_idx])\n",
    "            model = xgb.train(xgb_p, dm_tr, num_boost_round=500,\n",
    "                              evals=[(dm_val, 'val')], verbose_eval=0, early_stopping_rounds=30)\n",
    "            oof_l2_xgb[val_idx, i] = model.predict(dm_val)\n",
    "            test_l2_xgb[:, i] += model.predict(dm_te) / N_FOLDS_L2\n",
    "            del model, dm_tr, dm_val\n",
    "        del dm_te; gc.collect()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            auc = roc_auc_score(y_col, oof_l2_xgb[:, i])\n",
    "            log_msg(f\"  [{i+1}/41] {tcol}: AUC={auc:.4f}\")\n",
    "\n",
    "    np.save(f'{ART_L2}/oof_l2_xgb.npy', oof_l2_xgb)\n",
    "    np.save(f'{ART_L2}/test_l2_xgb.npy', test_l2_xgb)\n",
    "    macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_xgb[:, i]) for i in range(41)])\n",
    "    log_msg(f\"L2 XGB OOF Macro AUC: {macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: L2 NN v2 (SKIP if artifacts exist)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for v3 first, then v2\n",
    "nn_version = None\n",
    "if os.path.exists(f'{ART_L2}/oof_l2_nn_v3.npy'):\n",
    "    nn_version = 'v3'\n",
    "elif os.path.exists(f'{ART_L2}/oof_l2_nn_v2.npy'):\n",
    "    nn_version = 'v2'\n",
    "\n",
    "if nn_version:\n",
    "    print(f\">>> L2 NN: SKIP (артефакты {nn_version} найдены) <<<\")\n",
    "    oof_l2_nn = np.load(f'{ART_L2}/oof_l2_nn_{nn_version}.npy')\n",
    "    test_l2_nn = np.load(f'{ART_L2}/test_l2_nn_{nn_version}.npy')\n",
    "    macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_nn[:, i]) for i in range(41)])\n",
    "    print(f\"  OOF Macro AUC: {macro:.4f}\")\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"PyTorch device: {device}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_l2_train_scaled = scaler.fit_transform(X_l2_train).astype(np.float32)\n",
    "    X_l2_test_scaled = scaler.transform(X_l2_test).astype(np.float32)\n",
    "    print(f\"Scaled range: [{X_l2_train_scaled.min():.2f}, {X_l2_train_scaled.max():.2f}]\")\n",
    "\n",
    "    class L2NetV2(nn.Module):\n",
    "        def __init__(self, in_dim=205, h1=512, h2=256, h3=128, n_targets=41,\n",
    "                     drop1=0.3, drop2=0.25, drop3=0.2):\n",
    "            super().__init__()\n",
    "            self.input_norm = nn.LayerNorm(in_dim)\n",
    "            self.fc1 = nn.Linear(in_dim, h1)\n",
    "            self.ln1 = nn.LayerNorm(h1)\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.ln2 = nn.LayerNorm(h2)\n",
    "            self.skip_proj = nn.Linear(h1, h2)\n",
    "            self.fc3 = nn.Linear(h2, h3)\n",
    "            self.ln3 = nn.LayerNorm(h3)\n",
    "            self.classifier = nn.Linear(h3, n_targets)\n",
    "            self.drop1 = nn.Dropout(drop1)\n",
    "            self.drop2 = nn.Dropout(drop2)\n",
    "            self.drop3 = nn.Dropout(drop3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.input_norm(x)\n",
    "            h1 = self.drop1(F.silu(self.ln1(self.fc1(x))))\n",
    "            h2 = self.ln2(self.fc2(h1))\n",
    "            h2 = self.drop2(F.silu(h2 + self.skip_proj(h1) * 0.5))\n",
    "            h3 = self.drop3(F.silu(self.ln3(self.fc3(h2))))\n",
    "            return self.classifier(h3)\n",
    "\n",
    "    N_EPOCHS = 60; BATCH = 512; LR = 0.001; WD = 1e-5; PATIENCE = 15\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"L2 NN v3: {N_FOLDS_L2}-fold, {N_EPOCHS} ep, patience={PATIENCE}, batch={BATCH}, lr={LR}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    oof_l2_nn = np.zeros((len(X_l2_train), 41), dtype=np.float32)\n",
    "    test_l2_nn = np.zeros((len(X_l2_test), 41), dtype=np.float32)\n",
    "    X_te_tensor = torch.FloatTensor(X_l2_test_scaled).to(device)\n",
    "    y_tensor_all = torch.FloatTensor(y_train_arr.astype(np.float32))\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS_L2, shuffle=True, random_state=RANDOM_SEED)\n",
    "    t_start = time.time()\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_l2_train_scaled, y_train_arr[:, 0])):\n",
    "        t0 = time.time()\n",
    "        X_tr = torch.FloatTensor(X_l2_train_scaled[tr_idx]).to(device)\n",
    "        y_tr = y_tensor_all[tr_idx].to(device)\n",
    "        X_val = torch.FloatTensor(X_l2_train_scaled[val_idx]).to(device)\n",
    "        train_dl = DataLoader(TensorDataset(X_tr, y_tr), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "        model = L2NetV2().to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=LR, epochs=N_EPOCHS,\n",
    "            steps_per_epoch=len(train_dl), pct_start=0.3)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        best_auc = 0; best_state = None; no_improve = 0\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            model.train()\n",
    "            for xb, yb in train_dl:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_probs = torch.sigmoid(model(X_val)).cpu().numpy()\n",
    "            aucs = [roc_auc_score(y_train_arr[val_idx, j], val_probs[:, j]) for j in range(41)]\n",
    "            macro = np.mean(aucs)\n",
    "            if macro > best_auc:\n",
    "                best_auc = macro\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            oof_l2_nn[val_idx] = torch.sigmoid(model(X_val)).cpu().numpy()\n",
    "            test_l2_nn += torch.sigmoid(model(X_te_tensor)).cpu().numpy() / N_FOLDS_L2\n",
    "        log_msg(f\"Fold {fold+1}: AUC={best_auc:.4f}, best_ep={epoch+1-no_improve}, stopped={epoch+1}, time={time.time()-t0:.0f}s\")\n",
    "        del model, X_tr, y_tr, X_val, best_state\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    del X_te_tensor; torch.cuda.empty_cache()\n",
    "    macro_nn = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_nn[:, i]) for i in range(41)])\n",
    "    log_msg(f\"L2 NN OOF Macro AUC: {macro_nn:.4f}, time: {(time.time()-t_start)/60:.1f} min\")\n",
    "    np.save(f'{ART_L2}/oof_l2_nn_v3.npy', oof_l2_nn)\n",
    "    np.save(f'{ART_L2}/test_l2_nn_v3.npy', test_l2_nn)\n",
    "    print(f\"Saved oof_l2_nn_v3.npy, test_l2_nn_v3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Final Blend + Submission\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL BLEND\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test alphas\n",
    "alphas = [1.0, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50]\n",
    "best_alpha, best_macro = 1.0, 0\n",
    "\n",
    "for alpha in alphas:\n",
    "    blend = alpha * oof_l2_xgb + (1 - alpha) * oof_l2_nn\n",
    "    aucs = [roc_auc_score(y_train_arr[:, i], blend[:, i]) for i in range(41)]\n",
    "    macro = np.mean(aucs)\n",
    "    marker = ' <<< BEST' if macro > best_macro else ''\n",
    "    print(f\"  XGB {alpha*100:3.0f}% / NN {(1-alpha)*100:3.0f}%: Macro AUC = {macro:.4f}{marker}\")\n",
    "    if macro > best_macro:\n",
    "        best_macro = macro; best_alpha = alpha\n",
    "\n",
    "print(f\"\\nBest: alpha={best_alpha:.2f}, OOF Macro AUC = {best_macro:.4f}\")\n",
    "\n",
    "# --- Submission ---\n",
    "blend_test = (best_alpha * test_l2_xgb + (1 - best_alpha) * test_l2_nn).astype(np.float64)\n",
    "\n",
    "test_main = pd.read_parquet(f'{DATA}/test_main_features.parquet', columns=['customer_id'])\n",
    "predict_cols = [c.replace('target_', 'predict_') for c in target_cols]\n",
    "\n",
    "submission = pd.DataFrame({'customer_id': test_main['customer_id'].values})\n",
    "for i, pcol in enumerate(predict_cols):\n",
    "    submission[pcol] = blend_test[:, i]\n",
    "\n",
    "sub_path = f'{DATA}/submissions/sub_exp014_blend.parquet'\n",
    "os.makedirs(f'{DATA}/submissions', exist_ok=True)\n",
    "submission.to_parquet(sub_path, index=False)\n",
    "\n",
    "# Validation\n",
    "print(f\"\\n--- Submission validation ---\")\n",
    "print(f\"Shape:   {submission.shape}\")\n",
    "print(f\"Columns: {list(submission.columns[:4])}... (predict_*, NOT target_*)\")\n",
    "print(f\"Dtype:   {submission[predict_cols[0]].dtype} (must be float64)\")\n",
    "print(f\"Range:   [{submission[predict_cols].min().min():.6f}, {submission[predict_cols].max().max():.6f}]\")\n",
    "print(f\"NaN:     {submission[predict_cols].isna().any().any()}\")\n",
    "print(f\"\\nSaved: {sub_path}\")\n",
    "print(f\"\\n>>> DONE! OOF={best_macro:.4f}, expected LB ~0.8515 <<<\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}