{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-012b: PyTorch L2 + XGBoost L2 Blend\n",
    "\n",
    "**LB: 0.8510** (лучший результат)\n",
    "\n",
    "## Пайплайн\n",
    "1. Загружаем L1 OOF (XGBoost per-target, Optuna params, 5-fold) — `xgb_oof_optuna.npy`\n",
    "2. Logit-трансформация OOF для NN\n",
    "3. Full train Multi-Task NN со skip connection на logit(OOF)\n",
    "4. Бленд 60% XGBoost L2 (стекинг) + 40% PyTorch L2\n",
    "\n",
    "## Артефакты (входные)\n",
    "- `xgb_oof_optuna.npy` — L1 OOF [750k × 41]\n",
    "- `xgb_test_optuna.npy` — L1 test preds [250k × 41]\n",
    "- `train_target.parquet` — таргеты\n",
    "- `submission_optuna_stacking.parquet` — L2 XGBoost стекинг (LB 0.8472)\n",
    "\n",
    "## Архитектура NN\n",
    "```\n",
    "Input(41) → Linear(41→512) → BN → SiLU → Drop(0.19)\n",
    "         → Linear(512→256) → BN → SiLU → Drop(0.22)\n",
    "         → [concat с input: 256+41=297]\n",
    "         → Linear(297→41) → sigmoid\n",
    "```\n",
    "Skip connection: вход конкатенируется с выходом feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 0: Настройка окружения\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Путь к данным (Google Drive)\n",
    "DATA = '/content/drive/MyDrive/data_fusion'  # <- поменять при необходимости\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: Загрузка данных и logit-трансформация\n",
    "\n",
    "OOF_TRAIN_PATH = f'{DATA}/xgb_oof_optuna.npy'       # L1 OOF (750000, 41)\n",
    "TEST_PREDS_PATH = f'{DATA}/xgb_test_optuna.npy'     # L1 Test (250000, 41)\n",
    "TARGET_PATH = f'{DATA}/train_target.parquet'\n",
    "BEST_SUB_PATH = f'{DATA}/submission_optuna_stacking.parquet'  # XGB L2, LB 0.8472\n",
    "\n",
    "print('Загрузка данных...')\n",
    "X_meta_train = np.load(OOF_TRAIN_PATH).astype(np.float32)\n",
    "X_meta_test = np.load(TEST_PREDS_PATH).astype(np.float32)\n",
    "\n",
    "targets_df = pd.read_parquet(TARGET_PATH)\n",
    "if 'customer_id' in targets_df.columns:\n",
    "    targets_df = targets_df.drop('customer_id', axis=1)\n",
    "y_meta_train = targets_df.values.astype(np.float32)\n",
    "\n",
    "# Logit-трансформация: log(p/(1-p)) — нормализует вероятности для NN\n",
    "eps = 1e-7\n",
    "X_meta_train = np.clip(X_meta_train, eps, 1 - eps)\n",
    "X_meta_test = np.clip(X_meta_test, eps, 1 - eps)\n",
    "\n",
    "X_train_logits = np.log(X_meta_train / (1 - X_meta_train))\n",
    "X_test_logits = np.log(X_meta_test / (1 - X_meta_test))\n",
    "\n",
    "print(f'Train: {X_train_logits.shape}, Test: {X_test_logits.shape}')\n",
    "print(f'Targets: {y_meta_train.shape}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: Перенос данных на GPU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "X_train_t = torch.tensor(X_train_logits, dtype=torch.float32, device=device)\n",
    "y_train_t = torch.tensor(y_meta_train, dtype=torch.float32, device=device)\n",
    "X_test_t = torch.tensor(X_test_logits, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f'GPU memory: {torch.cuda.memory_allocated()/1e6:.0f} MB')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 3: Архитектура Multi-Task NN со skip connection\n",
    "# Optuna-параметры: hidden=512, drop1=0.1907, drop2=0.2226, lr=0.02558, wd=1.157e-5\n",
    "\n",
    "class MultiLabelMetaNN_Res(nn.Module):\n",
    "    \"\"\"Multi-Task NN с skip connection для L2 мета-обучения.\n",
    "    \n",
    "    Вход: logit(OOF) [batch, 41]\n",
    "    Выход: logits [batch, 41] (до sigmoid)\n",
    "    \n",
    "    Skip connection: исходный вход конкатенируется с выходом feature_extractor,\n",
    "    позволяя classifier видеть и трансформированные, и сырые признаки.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=41, hidden_dim=512, drop_1=0.1907, drop_2=0.2226, output_dim=41):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(drop_1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(drop_2)\n",
    "        )\n",
    "        self.classifier = nn.Linear((hidden_dim // 2) + input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        combined = torch.cat([features, x], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "print('Архитектура:')\n",
    "model = MultiLabelMetaNN_Res().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Параметров: {total_params:,}')\n",
    "print(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 4: Full Train (750k, 100 эпох)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4096\n",
    "n_samples = X_train_t.shape[0]\n",
    "n_batches = int(np.ceil(n_samples / BATCH_SIZE))\n",
    "\n",
    "model = MultiLabelMetaNN_Res().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.02558/10, weight_decay=1.157e-05)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.02558, steps_per_epoch=n_batches, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(f'Старт обучения: {EPOCHS} эпох, batch={BATCH_SIZE}, lr=0.02558')\n",
    "start_time = time.time()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    perm = torch.randperm(n_samples, device=device)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(0, n_samples, BATCH_SIZE):\n",
    "        idx = perm[i:i+BATCH_SIZE]\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_t[idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss/n_batches:.4f}')\n",
    "\n",
    "print(f'Обучение завершено за {time.time() - start_time:.2f} сек!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5: Инференс на test + бленд с XGB L2\n",
    "\n",
    "print('Генерация предсказаний для теста...')\n",
    "model.eval()\n",
    "nn_test_preds = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, X_test_t.shape[0], BATCH_SIZE):\n",
    "        X_batch = X_test_t[i:i+BATCH_SIZE]\n",
    "        logits = model(X_batch)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        nn_test_preds.append(probs.cpu().numpy())\n",
    "nn_test_preds = np.vstack(nn_test_preds)\n",
    "\n",
    "# Загружаем лучший XGB L2 сабмит\n",
    "best_sub = pd.read_parquet(BEST_SUB_PATH)\n",
    "customer_ids = best_sub['customer_id'].values\n",
    "best_sub_preds = best_sub.drop('customer_id', axis=1).values\n",
    "\n",
    "# Бленд: 60% XGB L2 + 40% NN L2\n",
    "blend_test_preds = 0.6 * best_sub_preds + 0.4 * nn_test_preds\n",
    "\n",
    "# Формирование сабмита\n",
    "submit_cols = [c for c in best_sub.columns if c != 'customer_id']\n",
    "final_sub = pd.DataFrame(blend_test_preds, columns=submit_cols)\n",
    "final_sub.insert(0, 'customer_id', customer_ids)\n",
    "final_sub['customer_id'] = final_sub['customer_id'].astype(np.int32)\n",
    "\n",
    "sub_name = 'submission_EXP012b_nn_blend.parquet'\n",
    "final_sub.to_parquet(sub_name, index=False)\n",
    "print(f'Сабмит сохранён: {sub_name}')\n",
    "print(f'Shape: {final_sub.shape}')\n",
    "print(f'\\nСтатистика предсказаний:')\n",
    "print(f'  Mean: {blend_test_preds.mean():.4f}')\n",
    "print(f'  Min:  {blend_test_preds.min():.6f}')\n",
    "print(f'  Max:  {blend_test_preds.max():.6f}')"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
