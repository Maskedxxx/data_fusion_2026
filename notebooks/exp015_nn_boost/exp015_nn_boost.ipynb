{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-015: NN L2 Boost (RankGauss + Input Dropout)\n",
    "\n",
    "**Базируется на EXP-014** (LB 0.8522)\n",
    "\n",
    "## Что нового:\n",
    "- **NN v4**: RankGauss (QuantileTransformer) вместо StandardScaler + Input Dropout 0.10\n",
    "- **3-way blend**: XGB 55% + NN_v3 20% + NN_v4 25% (diversity от двух разных скалеров)\n",
    "\n",
    "## Результат:\n",
    "- NN v4 OOF: **0.8426** (v3 was 0.8415, **+0.0011**)\n",
    "- 3-way blend OOF: **0.8487** (vs 2-way 0.8482, **+0.0005**)\n",
    "- **LB: 0.8522** (паритет с EXP-014)\n",
    "\n",
    "## Ключевой инсайт:\n",
    "RankGauss дал +0.0011 OOF для NN (32/41 таргетов улучшились), но 3-way blend\n",
    "эффективнее 2-way: v3 и v4 обучены на разных скалерах → разные ошибки → diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup + Load EXP-014 artifacts\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc, time, os, json\n",
    "from datetime import datetime\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "log_msg = lambda msg: print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "# --- Пути (Drive папка = data_fusion, НЕ data_fusion_2026!) ---\n",
    "DATA = '/content/drive/MyDrive/data_fusion'\n",
    "ART_L1 = f'{DATA}/artifacts/l1_oof'\n",
    "ART_L2 = f'{DATA}/artifacts/l2_stacking'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS_L2 = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# --- Таргеты (БЕЗ sorted! sorted ломает порядок столбцов → AUC=0.50) ---\n",
    "target = pd.read_parquet(f'{DATA}/train_target.parquet')\n",
    "target_cols = [c for c in target.columns if c.startswith('target_')]\n",
    "y_train_arr = target[target_cols].values.astype(np.int8)\n",
    "train_ids = target['customer_id'].values\n",
    "del target; gc.collect()\n",
    "print(f\"Targets: {y_train_arr.shape}, cols: {len(target_cols)}\")\n",
    "print(f\"Порядок: {target_cols[:3]} ... {target_cols[-2:]}\")\n",
    "\n",
    "# --- L1 OOF ---\n",
    "oof_xgb = np.load(f'{ART_L1}/oof_xgb.npy')\n",
    "oof_cb  = np.load(f'{ART_L1}/oof_cb.npy')\n",
    "oof_lgb = np.load(f'{ART_L1}/oof_lgb.npy')\n",
    "test_xgb = np.load(f'{ART_L1}/test_xgb.npy')\n",
    "test_cb  = np.load(f'{ART_L1}/test_cb.npy')\n",
    "test_lgb = np.load(f'{ART_L1}/test_lgb.npy')\n",
    "print(f\"L1 OOF: XGB {oof_xgb.shape}, CB {oof_cb.shape}, LGB {oof_lgb.shape}\")\n",
    "\n",
    "# --- L2 матрица (123 OOF + 82 meta = 205 features) ---\n",
    "X_l2_train = np.hstack([oof_xgb, oof_cb, oof_lgb])\n",
    "X_l2_test  = np.hstack([test_xgb, test_cb, test_lgb])\n",
    "oof_stack = np.stack([oof_xgb, oof_cb, oof_lgb], axis=0)\n",
    "test_stack = np.stack([test_xgb, test_cb, test_lgb], axis=0)\n",
    "X_l2_train = np.hstack([X_l2_train, oof_stack.mean(0), oof_stack.std(0)])\n",
    "X_l2_test  = np.hstack([X_l2_test, test_stack.mean(0), test_stack.std(0)])\n",
    "del oof_stack, test_stack; gc.collect()\n",
    "print(f\"L2 matrix: train {X_l2_train.shape}, test {X_l2_test.shape}\")\n",
    "\n",
    "# --- L2 XGB OOF (якорь) ---\n",
    "oof_l2_xgb = np.load(f'{ART_L2}/oof_l2_xgb.npy')\n",
    "test_l2_xgb = np.load(f'{ART_L2}/test_l2_xgb.npy')\n",
    "xgb_macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_xgb[:, i]) for i in range(41)])\n",
    "print(f\"L2 XGB OOF Macro AUC: {xgb_macro:.4f}\")\n",
    "\n",
    "# --- L2 NN v3 OOF (файл oof_l2_nn_v3.npy, НЕ oof_l2_nn.npy!) ---\n",
    "oof_l2_nn_v3 = np.load(f'{ART_L2}/oof_l2_nn_v3.npy')\n",
    "test_l2_nn_v3 = np.load(f'{ART_L2}/test_l2_nn_v3.npy')\n",
    "nn_v3_macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_nn_v3[:, i]) for i in range(41)])\n",
    "print(f\"L2 NN v3 OOF Macro AUC: {nn_v3_macro:.4f}\")\n",
    "\n",
    "# --- Blend baseline ---\n",
    "blend = 0.6 * oof_l2_xgb + 0.4 * oof_l2_nn_v3\n",
    "blend_macro = np.mean([roc_auc_score(y_train_arr[:, i], blend[:, i]) for i in range(41)])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BASELINE: XGB={xgb_macro:.4f}, NN_v3={nn_v3_macro:.4f}, Blend 60/40={blend_macro:.4f}\")\n",
    "print(f\"LB 0.8522 (рекорд)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: NN v4 — RankGauss + Input Dropout\n",
    "# ============================================================\n",
    "t_total = time.time()\n",
    "\n",
    "# --- RankGauss вместо StandardScaler ---\n",
    "log_msg(\"Fitting QuantileTransformer (RankGauss)...\")\n",
    "qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=RANDOM_SEED)\n",
    "X_train_rg = qt.fit_transform(X_l2_train).astype(np.float32)\n",
    "X_test_rg = qt.transform(X_l2_test).astype(np.float32)\n",
    "print(f\"RankGauss range: [{X_train_rg.min():.2f}, {X_train_rg.max():.2f}]\")\n",
    "\n",
    "# --- Модель с Input Dropout ---\n",
    "class L2NetV4(nn.Module):\n",
    "    \\\"\\\"\\\"v4 = v3 + RankGauss + Input Dropout 0.10\\\"\\\"\\\"\n",
    "    def __init__(self, in_dim=205, h1=512, h2=256, h3=128, n_targets=41,\n",
    "                 drop_input=0.10, drop1=0.3, drop2=0.25, drop3=0.2):\n",
    "        super().__init__()\n",
    "        self.input_drop = nn.Dropout(drop_input)\n",
    "        self.input_norm = nn.LayerNorm(in_dim)\n",
    "        self.fc1 = nn.Linear(in_dim, h1)\n",
    "        self.ln1 = nn.LayerNorm(h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.ln2 = nn.LayerNorm(h2)\n",
    "        self.skip_proj = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.ln3 = nn.LayerNorm(h3)\n",
    "        self.classifier = nn.Linear(h3, n_targets)\n",
    "        self.drop1 = nn.Dropout(drop1)\n",
    "        self.drop2 = nn.Dropout(drop2)\n",
    "        self.drop3 = nn.Dropout(drop3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_drop(x)\n",
    "        x = self.input_norm(x)\n",
    "        h1 = self.drop1(F.silu(self.ln1(self.fc1(x))))\n",
    "        h2 = self.ln2(self.fc2(h1))\n",
    "        h2 = self.drop2(F.silu(h2 + self.skip_proj(h1) * 0.5))\n",
    "        h3 = self.drop3(F.silu(self.ln3(self.fc3(h2))))\n",
    "        return self.classifier(h3)\n",
    "\n",
    "# --- Гиперпараметры ---\n",
    "N_EPOCHS = 60; BATCH = 512; LR = 0.001; WD = 1e-5; PATIENCE = 15\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"L2 NN v4: RankGauss + InputDrop(0.10)\")\n",
    "print(f\"{N_FOLDS_L2}-fold, {N_EPOCHS} ep, patience={PATIENCE}, batch={BATCH}, lr={LR}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "oof_l2_nn_v4 = np.zeros((len(X_train_rg), 41), dtype=np.float32)\n",
    "test_l2_nn_v4 = np.zeros((len(X_test_rg), 41), dtype=np.float32)\n",
    "X_te_tensor = torch.FloatTensor(X_test_rg).to(device)\n",
    "y_tensor_all = torch.FloatTensor(y_train_arr.astype(np.float32))\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS_L2, shuffle=True, random_state=RANDOM_SEED)\n",
    "fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_rg, y_train_arr[:, 0])):\n",
    "    t0 = time.time()\n",
    "    X_tr = torch.FloatTensor(X_train_rg[tr_idx]).to(device)\n",
    "    y_tr = y_tensor_all[tr_idx].to(device)\n",
    "    X_val = torch.FloatTensor(X_train_rg[val_idx]).to(device)\n",
    "    train_dl = DataLoader(TensorDataset(X_tr, y_tr), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "    model = L2NetV4().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LR, epochs=N_EPOCHS,\n",
    "        steps_per_epoch=len(train_dl), pct_start=0.3)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_auc = 0; best_state = None; no_improve = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_probs = torch.sigmoid(model(X_val)).cpu().numpy()\n",
    "        aucs = [roc_auc_score(y_train_arr[val_idx, j], val_probs[:, j]) for j in range(41)]\n",
    "        macro = np.mean(aucs)\n",
    "        if macro > best_auc:\n",
    "            best_auc = macro\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or no_improve >= PATIENCE:\n",
    "            log_msg(f\"  Fold {fold} ep {epoch+1}: AUC={macro:.4f} (best={best_auc:.4f}, no_imp={no_improve})\")\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        oof_l2_nn_v4[val_idx] = torch.sigmoid(model(X_val)).cpu().numpy()\n",
    "        test_l2_nn_v4 += torch.sigmoid(model(X_te_tensor)).cpu().numpy() / N_FOLDS_L2\n",
    "\n",
    "    fold_aucs.append(best_auc)\n",
    "    log_msg(f\"Fold {fold}: AUC={best_auc:.4f}, time={time.time()-t0:.0f}s\")\n",
    "    del X_tr, y_tr, X_val, model, optimizer, scheduler, best_state\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# --- Результат ---\n",
    "v4_macro = np.mean([roc_auc_score(y_train_arr[:, i], oof_l2_nn_v4[:, i]) for i in range(41)])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"NN v4 OOF Macro AUC: {v4_macro:.4f} (v3 was {nn_v3_macro:.4f}, diff={v4_macro-nn_v3_macro:+.4f})\")\n",
    "print(f\"Per-fold: {[f'{a:.4f}' for a in fold_aucs]}\")\n",
    "\n",
    "# Blend с XGB\n",
    "for w in [0.5, 0.6, 0.7]:\n",
    "    bl = w * oof_l2_xgb + (1-w) * oof_l2_nn_v4\n",
    "    bl_auc = np.mean([roc_auc_score(y_train_arr[:, i], bl[:, i]) for i in range(41)])\n",
    "    print(f\"Blend {w:.0%}/{1-w:.0%} XGB+NNv4: {bl_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time()-t_total)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: 3-way blend analysis + save v4\n",
    "# ============================================================\n",
    "\n",
    "print(\"=== 3-way blend: XGB + NN_v3 + NN_v4 ===\")\n",
    "for w_xgb in [0.55, 0.60, 0.65]:\n",
    "    for w_v3 in [0.10, 0.15, 0.20]:\n",
    "        w_v4 = round(1.0 - w_xgb - w_v3, 2)\n",
    "        if w_v4 < 0.05:\n",
    "            continue\n",
    "        bl = w_xgb * oof_l2_xgb + w_v3 * oof_l2_nn_v3 + w_v4 * oof_l2_nn_v4\n",
    "        auc = np.mean([roc_auc_score(y_train_arr[:, i], bl[:, i]) for i in range(41)])\n",
    "        tag = \" <<<\" if auc > blend_macro + 0.00005 else \"\"\n",
    "        print(f\"  XGB={w_xgb:.2f} v3={w_v3:.2f} v4={w_v4:.2f} -> OOF={auc:.4f} (diff={auc-blend_macro:+.5f}){tag}\")\n",
    "\n",
    "# Per-target comparison\n",
    "print(f\"\\n=== Per-target: v4 vs v3 ===\")\n",
    "better = 0; worse = 0\n",
    "for i, tc in enumerate(target_cols):\n",
    "    a3 = roc_auc_score(y_train_arr[:, i], oof_l2_nn_v3[:, i])\n",
    "    a4 = roc_auc_score(y_train_arr[:, i], oof_l2_nn_v4[:, i])\n",
    "    diff = a4 - a3\n",
    "    if abs(diff) > 0.002:\n",
    "        print(f\"  {tc}: v3={a3:.4f} v4={a4:.4f} diff={diff:+.4f}\")\n",
    "    if diff > 0: better += 1\n",
    "    else: worse += 1\n",
    "print(f\"v4 лучше: {better}/41, v4 хуже: {worse}/41\")\n",
    "\n",
    "# Save v4\n",
    "np.save(f'{ART_L2}/oof_l2_nn_v4.npy', oof_l2_nn_v4)\n",
    "np.save(f'{ART_L2}/test_l2_nn_v4.npy', test_l2_nn_v4)\n",
    "print(f\"\\nv4 артефакты сохранены в {ART_L2}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Submission — 3-way blend\n",
    "# ============================================================\n",
    "\n",
    "W_XGB, W_V3, W_V4 = 0.55, 0.20, 0.25\n",
    "\n",
    "test_blend = W_XGB * test_l2_xgb + W_V3 * test_l2_nn_v3 + W_V4 * test_l2_nn_v4\n",
    "print(f\"Test blend: XGB={W_XGB}, v3={W_V3}, v4={W_V4}\")\n",
    "print(f\"Test range: [{test_blend.min():.6f}, {test_blend.max():.6f}]\")\n",
    "\n",
    "test_df = pd.read_parquet(f'{DATA}/test_main_features.parquet', columns=['customer_id'])\n",
    "sub = pd.DataFrame({'customer_id': test_df['customer_id'].values})\n",
    "for i, tc in enumerate(target_cols):\n",
    "    pred_col = tc.replace('target_', 'predict_')\n",
    "    sub[pred_col] = test_blend[:, i].astype(np.float64)\n",
    "\n",
    "assert sub.shape == (250000, 42)\n",
    "assert all(c.startswith('predict_') for c in sub.columns[1:])\n",
    "assert sub.iloc[:, 1:].dtypes.unique()[0] == np.float64\n",
    "\n",
    "out_path = f'{DATA}/submission_exp015_3way_blend.parquet'\n",
    "sub.to_parquet(out_path, index=False)\n",
    "print(f\"\\nСохранено: {out_path}\")\n",
    "print(f\"OOF: 0.8487 (vs baseline 0.8482, +0.0005)\")\n",
    "print(f\"LB: 0.8522\")\n",
    "\n",
    "del test_df; gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}