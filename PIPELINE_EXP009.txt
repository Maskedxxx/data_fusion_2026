╔══════════════════════════════════════════════════════════════════════════════════╗
║           EXP-009: СТЕКИНГ ПАЙПЛАЙН — PUBLIC LB 0.8444 (ЛУЧШИЙ)               ║
║                                                                                ║
║  Двухуровневая архитектура: L1 (41 XGBoost) → L2 (41 мета-модели XGBoost)     ║
║  Метрика: macro ROC-AUC по 41 таргету                                          ║
╚══════════════════════════════════════════════════════════════════════════════════╝


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 1: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
═══════════════════════════════════════════════════════════════════════════════════

  train_main.parquet          train_extra.parquet         train_target.parquet
  ┌──────────────────┐        ┌──────────────────┐        ┌──────────────────┐
  │ 750k строк       │        │ 750k строк       │        │ 750k строк       │
  │ customer_id      │        │ customer_id      │        │ customer_id      │
  │ 67 cat_feature_* │        │ 2241 числовых    │        │ 41 target_*_*    │
  │ 132 num_feature_*│        │ (обфусцировано)  │        │ (0 или 1)        │
  │ = 199 признаков  │        │                  │        │                  │
  └────────┬─────────┘        └────────┬─────────┘        └────────┬─────────┘
           │                           │                           │
           └─────────┬─────────────────┘                           │
                     ▼                                             ▼
              LEFT JOIN по customer_id                     drop customer_id
                     │                                             │
                     ▼                                             ▼
           ┌──────────────────┐                          ┌──────────────────┐
           │ X_train_np       │                          │ y_train_np       │
           │ (750000, 2440)   │                          │ (750000, 41)     │
           │ float32          │                          │ int (0/1)        │
           │ drop customer_id │                          │                  │
           └──────────────────┘                          └──────────────────┘

  test_main.parquet           test_extra.parquet
  ┌──────────────────┐        ┌──────────────────┐
  │ 250k строк       │        │ 250k строк       │
  │ 199 признаков    │        │ 2241 числовых    │
  └────────┬─────────┘        └────────┬─────────┘
           └─────────┬─────────────────┘
                     ▼
           ┌──────────────────┐        ┌──────────────────┐
           │ X_test_np        │        │ test_customer_ids │
           │ (250000, 2440)   │        │ (250000,)         │
           │ float32          │        │ для сабмита       │
           └──────────────────┘        └──────────────────┘

  Нюансы:
  • astype(np.float32) — категориальные тоже переводятся во float для XGBoost GPU
  • Все пропуски (NaN) сохраняются — XGBoost обрабатывает их нативно (learn_missing)
  • gc.collect() — освобождаем pandas DataFrames (~3 GB)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 2: УРОВЕНЬ L1 — ГЕНЕРАЦИЯ OOF ПРЕДСКАЗАНИЙ (750k × 41)
═══════════════════════════════════════════════════════════════════════════════════

  Цикл повторяется 41 раз — по одному для каждого таргета (target_1_1 ... target_10_1)
  Каждый таргет обучается НЕЗАВИСИМО, со своими фичами

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                     ДЛЯ КАЖДОГО ТАРГЕТА t (1..41):                        │
  │                                                                           │
  │  ┌───────────────────────────────────────────────────────────────────────┐ │
  │  │  ЭТАП A: ЧЕРНОВИК (Feature Selection)                                │ │
  │  │                                                                      │ │
  │  │  X_train_np (750k, 2440)  +  y_target (750k,)                       │ │
  │  │  ВСЕ 2440 признаков            столбец t из y_train_np              │ │
  │  │         │                              │                             │ │
  │  │         ▼                              ▼                             │ │
  │  │  QuantileDMatrix (мастер-матрица, строится 1 раз для всех таргетов)  │ │
  │  │  • set_label(y_target) — меняем только метки, матрица та же          │ │
  │  │  • QuantileDMatrix: GPU-оптимизированный формат, квантильные гисто-  │ │
  │  │    граммы вместо точных значений (экономит память, ускоряет в 2-3×)   │ │
  │  │         │                                                            │ │
  │  │         ▼                                                            │ │
  │  │  XGBoost обучение (100 деревьев — быстрый черновик)                  │ │
  │  │  ┌──────────────────────────────┐                                    │ │
  │  │  │ objective: binary:logistic   │                                    │ │
  │  │  │ eval_metric: auc             │                                    │ │
  │  │  │ lr: 0.05, max_depth: 6       │                                    │ │
  │  │  │ subsample: 0.8               │  ← 80% строк на каждое дерево     │ │
  │  │  │ colsample_bytree: 0.8        │  ← 80% столбцов на каждое дерево  │ │
  │  │  │ min_child_weight: 5 или 1    │  ← адаптивно (см. ниже)           │ │
  │  │  │ tree_method: hist, GPU       │                                    │ │
  │  │  │ num_boost_round: 100         │                                    │ │
  │  │  └──────────────────────────────┘                                    │ │
  │  │         │                                                            │ │
  │  │         ▼                                                            │ │
  │  │  model.get_score(importance_type='gain')                             │ │
  │  │  • gain = средний прирост метрики при сплите по этой фиче            │ │
  │  │  • XGBoost автоматически именует фичи f0, f1, ..., f2439             │ │
  │  │         │                                                            │ │
  │  │         ▼                                                            │ │
  │  │  Сортировка по gain (убывающая) + Cumulative Gain 95%                │ │
  │  │                                                                      │ │
  │  │  Пример для target_8_1:                                              │ │
  │  │  ┌────────────────┬────────┬──────────────┐                          │ │
  │  │  │ Фича           │ Gain   │ Cumul. Gain  │                          │ │
  │  │  ├────────────────┼────────┼──────────────┤                          │ │
  │  │  │ f22 (num_22)   │ 1177   │   12%  ✓     │                          │ │
  │  │  │ f156 (extra)   │  243   │   14%  ✓     │                          │ │
  │  │  │ f89 (num_89)   │  198   │   16%  ✓     │                          │ │
  │  │  │ ...            │ ...    │   ...  ✓     │                          │ │
  │  │  │ f1834          │    2   │   95%  ✓ ←── СТОП! Набрали 95%         │ │
  │  │  │ f901           │    1   │  95.1% ✗     │  ← отсечены             │ │
  │  │  │ ...            │  ...   │  ...   ✗     │                          │ │
  │  │  │ f2100          │    0   │  100%  ✗     │  ← gain=0, бесполезны   │ │
  │  │  └────────────────┴────────┴──────────────┘                          │ │
  │  │                                                                      │ │
  │  │  Результат: selected_indices для target_8_1 = [22, 156, 89, ...]     │ │
  │  │  Разные таргеты → разное кол-во фичей (273...898)                    │ │
  │  │                                                                      │ │
  │  │  Адаптивный min_child_weight:                                        │ │
  │  │  • pos > 500 → min_child_weight = 5 (стандартная защита от шума)     │ │
  │  │  • pos ≤ 500 → min_child_weight = 1 (разрешаем мелкие листья,       │ │
  │  │    иначе модель не может обучиться на единичных примерах)             │ │
  │  │                                                                      │ │
  │  │  Защита: если pos < 2 → таргет пропускается (OOF = 0.0001)          │ │
  │  └───────────────────────────────────────────────────────────────────────┘ │
  │                                                                           │
  │  ┌───────────────────────────────────────────────────────────────────────┐ │
  │  │  ЭТАП B: ЧИСТОВИК (5-Fold OOF)                                      │ │
  │  │                                                                      │ │
  │  │  Берём ТОЛЬКО отобранные фичи:                                       │ │
  │  │  X_target_np = X_train_np[:, selected_indices]                       │ │
  │  │  Например: (750000, 970) вместо (750000, 2440)                       │ │
  │  │  .copy(order='C') — C-contiguous для быстрого доступа GPU            │ │
  │  │                                                                      │ │
  │  │  StratifiedKFold(n_splits=5, shuffle=True, random_state=42)          │ │
  │  │  • Stratified = сохраняет пропорцию 0/1 в каждом фолде              │ │
  │  │  • Критично для редких таргетов (target_2_8: 83 позитива = ~17/фолд)│ │
  │  │                                                                      │ │
  │  │  ┌─────────────────────────────────────────────────────────────────┐  │ │
  │  │  │                                                                 │  │ │
  │  │  │  ФОЛД 1:   [████████████████████████████████████████]  750k    │  │ │
  │  │  │             ├─TRAIN (600k)───────────────┤├─VAL (150k)┤        │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  ФОЛД 2:   [████████████████████████████████████████]  750k    │  │ │
  │  │  │             ├─VAL─┤├─TRAIN (600k)────────────────────┤         │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  ФОЛД 3:   [████████████████████████████████████████]  750k    │  │ │
  │  │  │             ├─TRAIN─┤├─VAL─┤├─TRAIN──────────────────┤         │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  ФОЛД 4:   [████████████████████████████████████████]  750k    │  │ │
  │  │  │             ├─TRAIN──────────────┤├─VAL─┤├─TRAIN─────┤         │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  ФОЛД 5:   [████████████████████████████████████████]  750k    │  │ │
  │  │  │             ├─TRAIN (600k)───────────────────┤├─VAL──┤         │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  Для каждого фолда:                                             │  │ │
  │  │  │  1. dtrain = QuantileDMatrix(X_train_fold)  ← 600k × ~970      │  │ │
  │  │  │  2. dval   = QuantileDMatrix(X_val_fold, ref=dtrain)            │  │ │
  │  │  │     ref=dtrain → val использует те же квантильные бакеты        │  │ │
  │  │  │  3. XGBoost train: 500 деревьев (без early stopping!)           │  │ │
  │  │  │  4. model.predict(dval) → вероятности для val-части              │  │ │
  │  │  │                                                                 │  │ │
  │  │  │  КЛЮЧЕВОЙ МОМЕНТ: каждый клиент попадает в VAL ровно 1 раз     │  │ │
  │  │  │  → его предсказание "честное" (модель его НЕ ВИДЕЛА)            │  │ │
  │  │  │                                                                 │  │ │
  │  │  └─────────────────────────────────────────────────────────────────┘  │ │
  │  │                                                                      │ │
  │  │  oof_predictions[val_idx, t] = model.predict(dval)                   │ │
  │  │  Все 5 фолдов заполняют матрицу → у каждого клиента есть OOF        │ │
  │  └───────────────────────────────────────────────────────────────────────┘ │
  │                                                                           │
  │  Повторяем для всех 41 таргетов...                                        │
  └───────────────────────────────────────────────────────────────────────────┘

  РЕЗУЛЬТАТ ШАГА 2:

  oof_predictions (OOF-матрица)
  ┌──────────────────────────────────────────────────────────────┐
  │         target_1_1  target_1_2  ...  target_9_6  target_10_1│
  │ клиент 1   0.023       0.156   ...    0.341       0.089     │
  │ клиент 2   0.871       0.034   ...    0.112       0.456     │
  │ клиент 3   0.005       0.892   ...    0.678       0.234     │
  │ ...        ...         ...     ...    ...         ...       │
  │ клиент 750k 0.112      0.045   ...    0.223       0.567    │
  │                                                              │
  │ (750000, 41) float32                                         │
  │ Каждое значение — "честная" вероятность: модель НЕ видела    │
  │ этого клиента при обучении                                   │
  └──────────────────────────────────────────────────────────────┘
  Сохраняется: xgb_oof_train.npy

  best_features_dict = {
      "target_1_1": ["num_feature_22", "extra_feature_156", ...],  # 898 фичей
      "target_2_8": ["num_feature_22", "extra_feature_89", ...],   # 273 фичи
      ...                                                           # 41 список
  }
  Сохраняется: xgb_best_features.json

  OOF Macro AUC = 0.8352 (среднее AUC по 41 таргету)
  Время: ~118 мин (Colab T4) или ~90 мин (Spark)


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 3: FULL TRAIN + TEST INFERENCE (L1 предсказания на test)
═══════════════════════════════════════════════════════════════════════════════════

  Для каждого из 41 таргетов:

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  X_train_np[:, selected_indices]         X_test_np[:, selected_indices] │
  │  (750000, ~970)                          (250000, ~970)                 │
  │  ВСЕ 750k обучающих данных               Тестовые данные               │
  │         │                                        │                     │
  │         ▼                                        │                     │
  │  XGBoost train (500 iter)                        │                     │
  │  • Те же параметры что OOF                       │                     │
  │  • НО: обучение на ВСЕХ 750k                     │                     │
  │    (в OOF было 600k на фолд)                     │                     │
  │  • Без early stopping — фиксированные 500 iter   │                     │
  │         │                                        │                     │
  │         ▼                                        ▼                     │
  │      model ─────────────── predict ──────→ test_preds[:, t]            │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  Зачем нужен отдельный шаг 3 (а не взять модели из шага 2)?
  • В шаге 2 модели обучались на 600k (4/5 данных) — это для OOF
  • Здесь модели обучаются на ВСЕ 750k — используем максимум данных
  • Модели шага 2 нельзя использовать для теста, т.к. их 5 штук на таргет

  РЕЗУЛЬТАТ ШАГА 3:

  test_predictions (L1 test)
  ┌──────────────────────────────────────────────────────────────┐
  │         target_1_1  target_1_2  ...  target_9_6  target_10_1│
  │ тест 1     0.034       0.178   ...    0.289       0.123     │
  │ тест 2     0.912       0.056   ...    0.145       0.501     │
  │ ...        ...         ...     ...    ...         ...       │
  │ тест 250k  0.087      0.034   ...    0.198       0.412     │
  │                                                              │
  │ (250000, 41) float32                                         │
  └──────────────────────────────────────────────────────────────┘
  Сохраняется: xgb_test_preds.npy


═══════════════════════════════════════════════════════════════════════════════════
 ШАГ 4: УРОВЕНЬ L2 — МЕТА-МОДЕЛЬ (СТЕКИНГ)
═══════════════════════════════════════════════════════════════════════════════════

  Входные данные L2 — это ВЫХОДЫ L1:

  X_meta_train = xgb_oof_train.npy     (750000, 41)  ← OOF предсказания
  X_meta_test  = xgb_test_preds.npy    (250000, 41)  ← L1 test предсказания
  y_train      = train_target           (750000, 41)  ← настоящие метки

  Для каждого из 41 таргетов:

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  X_meta_train (750k, 41)                                               │
  │  ┌────────────────────────────────────────────────────┐                 │
  │  │ Признаки мета-модели — это OOF-вероятности:        │                 │
  │  │                                                    │                 │
  │  │ OOF_target_1_1  OOF_target_1_2  ...  OOF_target_10_1               │
  │  │     0.023           0.156       ...      0.089     │                 │
  │  │     0.871           0.034       ...      0.456     │                 │
  │  │     ...             ...         ...      ...       │                 │
  │  │                                                    │                 │
  │  │ Каждая строка — "портрет" клиента глазами L1:      │                 │
  │  │ насколько вероятно, что он купит КАЖДЫЙ из          │                 │
  │  │ 41 продуктов (по мнению базовых моделей)           │                 │
  │  └────────────────────────────────────────────────────┘                 │
  │         │                                                              │
  │         ▼                                                              │
  │  XGBClassifier — мета-модель для таргета t                             │
  │  ┌─────────────────────────────────────────┐                           │
  │  │ max_depth: 2         ← МЕЛКИЕ деревья!  │                           │
  │  │ n_estimators: 100    ← мало деревьев    │                           │
  │  │ lr: 0.05                                │                           │
  │  │                                         │                           │
  │  │ ПОЧЕМУ depth=2?                         │                           │
  │  │ • Всего 41 фича — нельзя глубоко        │                           │
  │  │ • Глубокие деревья переобучатся          │                           │
  │  │ • Нужны простые правила вида:            │                           │
  │  │   "если OOF_3_1 > 0.5 И OOF_3_2 > 0.3  │                           │
  │  │    → target_3_3 вероятен"                │                           │
  │  └─────────────────────────────────────────┘                           │
  │         │                                                              │
  │         ▼                                                              │
  │  clf.predict_proba(X_meta_test)[:, 1]                                  │
  │  final_test_preds[:, t] = вероятности для теста                        │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘

  ЧТО ВИДИТ МЕТА-МОДЕЛЬ (пример для target_3_3):

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                                                                        │
  │  Клиент А:  OOF_3_1=0.82  OOF_3_2=0.91  OOF_3_5=0.78  OOF_10_1=0.12 │
  │             ↑ группа 3 — все высокие       ↑ антагонист — низкий        │
  │             → мета-модель: target_3_3 ≈ 0.85 (высокая вероятность!)     │
  │                                                                        │
  │  Клиент Б:  OOF_3_1=0.05  OOF_3_2=0.03  OOF_3_5=0.02  OOF_10_1=0.89 │
  │             ↑ группа 3 — все низкие        ↑ антагонист — высокий       │
  │             → мета-модель: target_3_3 ≈ 0.01 (вероятность падает)       │
  │                                                                        │
  │  КОРРЕЛЯЦИИ, КОТОРЫЕ УЧИТ МЕТА-МОДЕЛЬ:                                 │
  │  • Группы 3-7: внутригрупповая корреляция (пакетные покупки)            │
  │  • target_10_1: антагонист (отрицательная корреляция с остальными)      │
  │  • Редкие таргеты (target_2_8, 83 позитива): получают сигнал            │
  │    от частых связанных (target_2_2, 40k позитивов)                      │
  │                                                                        │
  │  ДО стекинга: target_5_2 AUC = 0.71 (модель знает только свои фичи)   │
  │  ПОСЛЕ:       target_5_2 AUC = 0.79 (+0.08!)                           │
  │  Потому что мета-модель видит: "L1 считает что он купит 5_1 и 5_3      │
  │  → значит и 5_2 вероятен" (корреляция внутри группы)                   │
  │                                                                        │
  └──────────────────────────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════════
 ФИНАЛ: ФОРМИРОВАНИЕ САБМИТА
═══════════════════════════════════════════════════════════════════════════════════

  final_test_preds (250000, 41)
         │
         ▼
  submission = DataFrame:
  ┌─────────────┬──────────────┬──────────────┬───┬───────────────┐
  │ customer_id  │ predict_1_1  │ predict_1_2  │...│ predict_10_1  │
  ├─────────────┼──────────────┼──────────────┼───┼───────────────┤
  │ 12345        │ 0.0341...    │ 0.1782...    │   │ 0.1234...     │
  │ 67890        │ 0.9123...    │ 0.0561...    │   │ 0.5012...     │
  │ ...          │ ...          │ ...          │   │ ...           │
  └─────────────┴──────────────┴──────────────┴───┴───────────────┘
  • float64 (требование платформы)
  • Колонки: predict_X_Y (НЕ predict_target_X_Y!)
  • Формат: parquet


═══════════════════════════════════════════════════════════════════════════════════
 ОБЩАЯ СХЕМА ПОТОКА ДАННЫХ
═══════════════════════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                            УРОВЕНЬ L1 (41 модель)                         │
  │                                                                           │
  │ TRAIN (750k, 2440)                          TEST (250k, 2440)             │
  │       │                                           │                       │
  │       │   ┌──────────────────────────────┐        │                       │
  │       │   │  Per-target Feature Selection │        │                       │
  │       │   │  Черновик: 100 деревьев       │        │                       │
  │       │   │  → cumulative gain 95%        │        │                       │
  │       │   │  → 273..898 фичей на таргет   │        │                       │
  │       │   └──────────────┬───────────────┘        │                       │
  │       │                  │                        │                       │
  │       ▼                  ▼                        ▼                       │
  │  ┌─────────────┐  ┌─────────────┐          ┌─────────────┐               │
  │  │ 5-Fold OOF  │  │  Full Train │          │             │               │
  │  │ 500 iter    │  │  750k       │──predict─→│ test preds  │               │
  │  │ per target  │  │  500 iter   │          │ (250k, 41)  │               │
  │  └──────┬──────┘  └─────────────┘          └──────┬──────┘               │
  │         │                                         │                       │
  │         ▼                                         │                       │
  │  OOF матрица                                      │                       │
  │  (750k, 41)                                       │                       │
  └────────┬──────────────────────────────────────────┬───────────────────────┘
           │                                          │
           │    "Честные" предсказания L1              │    Предсказания L1
           │    (модель не видела клиента)              │    (модель видела все 750k)
           │                                          │
  ┌────────▼──────────────────────────────────────────▼───────────────────────┐
  │                            УРОВЕНЬ L2 (41 мета-модель)                   │
  │                                                                          │
  │  X_meta_train = OOF (750k, 41)      X_meta_test = test_preds (250k, 41) │
  │  y_train = настоящие метки (750k, 41)                                    │
  │                                                                          │
  │  Для каждого таргета:                                                    │
  │    XGBClassifier(depth=2, 100 iter)                                      │
  │    fit(X_meta_train, y[:, t])                                            │
  │    predict_proba(X_meta_test) → final_preds[:, t]                        │
  │                                                                          │
  └───────────────────────────────┬──────────────────────────────────────────┘
                                  │
                                  ▼
                        final_test_preds (250k, 41)
                                  │
                                  ▼
                        submission.parquet → LB 0.8444


═══════════════════════════════════════════════════════════════════════════════════
 ПОЧЕМУ OOF, А НЕ ОБЫЧНЫЕ ПРЕДСКАЗАНИЯ?
═══════════════════════════════════════════════════════════════════════════════════

  ПРОБЛЕМА: если обучить L1 на train и предсказать train для мета-фичей

  ┌────────────────────────────────────────────────────────────────────────┐
  │  L1 обучается на train → предсказывает train → AUC ~0.99 (переобучен)│
  │  L2 обучается на этих "идеальных" предсказаниях                      │
  │  L2 думает: "если L1 говорит >0.9 — это точно позитив"               │
  │  НО на тесте L1 предсказывает хуже → L2 ломается                    │
  │                                                                      │
  │  Результат: УТЕЧКА (data leakage), мета-модель переобучена           │
  └────────────────────────────────────────────────────────────────────────┘

  РЕШЕНИЕ: Out-of-Fold (OOF)

  ┌────────────────────────────────────────────────────────────────────────┐
  │  L1 обучается на 4/5 train → предсказывает 1/5 (которые НЕ видел)   │
  │  OOF предсказания имеют такой же уровень ошибки, как на тесте        │
  │  L2 обучается на "реалистичных" предсказаниях                        │
  │  L2 корректно учит: "L1 ошибается на ±X, учитываю это"              │
  │                                                                      │
  │  Результат: нет утечки, мета-модель генерализуется                   │
  └────────────────────────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════════
 КЛЮЧЕВЫЕ НЮАНСЫ И ТРЮКИ
═══════════════════════════════════════════════════════════════════════════════════

  1. PER-TARGET FEATURE SELECTION
     • Каждый из 41 таргетов использует СВОЙ набор фичей
     • target_2_8 (83 позитива): 273 фичи — только самые сильные
     • target_8_1 (117k позитивов): 970 фичей — может извлечь больше сигнала
     • Зачем: шумные фичи мешают модели, особенно для редких таргетов

  2. АДАПТИВНЫЙ min_child_weight
     • Частые таргеты (>500 pos): min_child_weight=5 — защита от шума
     • Редкие таргеты (≤500 pos): min_child_weight=1 — каждый позитив ценен,
       нельзя запрещать мелкие листья

  3. QuantileDMatrix (GPU-оптимизация)
     • Вместо точных значений — квантильные бакеты (256 штук)
     • Мастер-матрица строится 1 раз, меняется только label
     • Экономит ~2 ГБ GPU памяти, ускоряет обучение в 2-3×

  4. ЧИСТОВИК БЕЗ EARLY STOPPING
     • Фиксированные 500 итераций для всех фолдов
     • Зачем: early stopping на val может давать разное кол-во деревьев
       для разных фолдов → нестабильные OOF
     • 500 — компромисс (EXP-007 показал: best_iter от 46 до 1546)

  5. SHALLOW META-MODEL (depth=2)
     • L2 видит только 41 фичу — мало для глубоких деревьев
     • depth=2 = правила вида "если A>x И B>y → класс 1"
     • Достаточно для захвата парных корреляций между таргетами
     • 100 деревьев × depth 2 = 100 простых правил

  6. ref=dtrain В QuantileDMatrix
     • dval строится с ref=dtrain → использует те же квантильные бакеты
     • Гарантирует: одинаковая дискретизация для train и val
     • Без ref: val может иметь другие бакеты → неконсистентные сплиты

  7. ЧЕКПОИНТЫ КАЖДЫЕ 5 ТАРГЕТОВ
     • np.save() после каждых 5 моделей
     • Защита от Colab-отключений (GPU лимит 4-12 часов)
     • Можно перезапустить и продолжить с последнего чекпоинта


═══════════════════════════════════════════════════════════════════════════════════
 ПАРАМЕТРЫ XGBoost (L1)
═══════════════════════════════════════════════════════════════════════════════════

  ┌────────────────────┬──────────┬────────────────────────────────────────────┐
  │ Параметр           │ Значение │ Смысл                                      │
  ├────────────────────┼──────────┼────────────────────────────────────────────┤
  │ objective          │ logistic │ Бинарная классификация → вероятность [0,1] │
  │ eval_metric        │ auc      │ ROC-AUC — совпадает с метрикой конкурса    │
  │ learning_rate      │ 0.05     │ Шаг обучения (маленький = стабильнее)      │
  │ max_depth          │ 6        │ Глубина дерева (6 = хороший баланс)        │
  │ subsample          │ 0.8      │ 80% строк на дерево (борьба с переобучен.) │
  │ colsample_bytree   │ 0.8      │ 80% фичей на дерево (разнообразие)         │
  │ min_child_weight   │ 5 / 1    │ Мин. сумма весов в листе (адаптивно)       │
  │ tree_method        │ hist     │ Гистограммный (быстрый, GPU-совместимый)   │
  │ device             │ cuda     │ GPU-ускорение                              │
  │ num_boost_round    │ 500      │ Кол-во деревьев (фиксированное)            │
  └────────────────────┴──────────┴────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════════
 TIMELINE (ДЛИТЕЛЬНОСТЬ)
═══════════════════════════════════════════════════════════════════════════════════

  Шаг 1: Загрузка данных .......................... ~2 мин
  Шаг 2: OOF 41 таргет × 5 фолдов × 500 iter ..... ~118 мин (Colab T4)
         (= 205 обучений XGBoost + 41 черновик)       ~90 мин (Spark)
  Шаг 3: Full train 41 × 500 iter + predict ....... ~45 мин
  Шаг 4: Мета-модель 41 × 100 iter ................ ~0.5 мин
                                                     ─────────
                                              ИТОГО: ~165 мин (~2.75 часа)
